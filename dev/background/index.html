<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Background Knowledge · OMEinsumContractionOrders.jl</title><meta name="title" content="Background Knowledge · OMEinsumContractionOrders.jl"/><meta property="og:title" content="Background Knowledge · OMEinsumContractionOrders.jl"/><meta property="twitter:title" content="Background Knowledge · OMEinsumContractionOrders.jl"/><meta name="description" content="Documentation for OMEinsumContractionOrders.jl."/><meta property="og:description" content="Documentation for OMEinsumContractionOrders.jl."/><meta property="twitter:description" content="Documentation for OMEinsumContractionOrders.jl."/><meta property="og:url" content="https://GiggleLiu.github.io/OMEinsumContractionOrders.jl/background/"/><meta property="twitter:url" content="https://GiggleLiu.github.io/OMEinsumContractionOrders.jl/background/"/><link rel="canonical" href="https://GiggleLiu.github.io/OMEinsumContractionOrders.jl/background/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/indigo.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">OMEinsumContractionOrders.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Background Knowledge</a><ul class="internal"><li><a class="tocitem" href="#Tensor-network"><span>Tensor network</span></a></li><li><a class="tocitem" href="#Einsum-notation"><span>Einsum notation</span></a></li><li><a class="tocitem" href="#Contraction-order-and-complexity"><span>Contraction order and complexity</span></a></li><li><a class="tocitem" href="#Tree-decomposition-and-tree-width"><span>Tree decomposition and tree width</span></a></li><li><a class="tocitem" href="#Reduce-space-complexity-by-slicing"><span>Reduce space complexity by slicing</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../tutorial/">Tutorial</a></li><li><a class="tocitem" href="../optimizers/">Choosing Optimizers</a></li><li><a class="tocitem" href="../ref/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Background Knowledge</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Background Knowledge</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/TensorBFS/OMEinsumContractionOrders.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/TensorBFS/OMEinsumContractionOrders.jl/blob/master/docs/src/background.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Tensor-Network-Contraction-Order-Optimization"><a class="docs-heading-anchor" href="#Tensor-Network-Contraction-Order-Optimization">Tensor Network Contraction Order Optimization</a><a id="Tensor-Network-Contraction-Order-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-Network-Contraction-Order-Optimization" title="Permalink"></a></h1><h2 id="Tensor-network"><a class="docs-heading-anchor" href="#Tensor-network">Tensor network</a><a id="Tensor-network-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-network" title="Permalink"></a></h2><p><em>Tensor network</em> is a powerful tool for modeling and simulating quantum many-body systems, probabilistic inference, combinatorial optimization, etc. It is a diagrammatic representation of tensor <em>contractions</em>. In this representation, a tensor is represented as a node, and an index is represented as a hyperedge (a hyperedge can connect to any number of nodes). For example, vectors, matrices and higher order tensors can be represented as:</p><p><img src="../assets/tensortypes.svg" alt/></p><p>To understand the basic concepts of tensor network, we recommend the following references for readers with different background:</p><ul><li>For readers with physics background: <a href="https://tensornetwork.org/diagrams/">https://tensornetwork.org/diagrams/</a></li><li>For readers want to get a formal definition: Chapter 2 of <a href="https://epubs.siam.org/doi/abs/10.1137/22M1501787">https://epubs.siam.org/doi/abs/10.1137/22M1501787</a></li></ul><p>The defining operation of a tensor network is <em>contraction</em>, which is also named as sum-product operation. It can be viewed as the generalization of matrix multiplication to tensors. We illustrate tensor network contraction with the following example.</p><h3 id="Example:-Trace-permutation-rule"><a class="docs-heading-anchor" href="#Example:-Trace-permutation-rule">Example: Trace permutation rule</a><a id="Example:-Trace-permutation-rule-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Trace-permutation-rule" title="Permalink"></a></h3><p>Let <span>$A, B$</span> and <span>$C$</span> be three square matrices with the same size. The trace operation <span>$\text{tr}(A B C)$</span> is defined as the sum of the products of the elements of the matrices:</p><p class="math-container">\[\text{tr}(A B C) = \sum_{i,j,k} A_{ij} B_{ik} C_{jk}.\]</p><p>It can be represented as a tensor network diagram as follows:</p><p><img src="../assets/traceperm.svg" alt/></p><p>So, each tensor is represented as a node, and each index is represented as an edge. Tensors sharing the same index are connected by the edge. The contraction of the tensor network is the summation of the products of the elements of the tensors over all possible values of the indices.</p><p>The contraction can happen in different orders. The famous trace permutation rule states that the trace is invariant under cyclic permutations of the matrices:</p><p class="math-container">\[\text{tr}(A B C) = \text{tr}(C A B) = \text{tr}(B C A).\]</p><p>Hence, we observe that tensor network contraction is associative and commutative, i.e., the order of evaluation does not change the result. In the tensor network diagram, the order of evaluation is neglected, so the trace permutation rule is trivially satisfied.</p><p>By drawing diagrams, we prove the trace permutation rule. Isn&#39;t it cool?</p><h2 id="Einsum-notation"><a class="docs-heading-anchor" href="#Einsum-notation">Einsum notation</a><a id="Einsum-notation-1"></a><a class="docs-heading-anchor-permalink" href="#Einsum-notation" title="Permalink"></a></h2><p>For simplicity, in the following sections, we will use the einsum notation in <a href="https://github.com/under-Peter/OMEinsum.jl">OMEinsum.jl</a> to represent the tensor network contraction. It is almost the same as the <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html">numpy einsum notation</a>. For example, the contraction of the tensor network in the previous trace permutation example can be represented as:</p><pre><code class="language-julia hljs">julia&gt; using OMEinsum

julia&gt; trace_perm = ein&quot;ij, jk, ki -&gt; &quot;
ij, jk, ki -&gt;</code></pre><p>The string <code>ij, jk, ki -&gt;</code> is the einsum notation, where labels of the input tensors and the output tensor are separated by <code>-&gt;</code>, and labels of different input tensors are separated by <code>,</code>. Here, <code>ij, jk, ki</code> are the labels of the input tensors of rank 2 (matrices), and the label of the output tensor is empty, representing a scalar.</p><p>Einsum notation is a powerful tool to represent linear operations, e.g. batched matrix multiplication is represented as <code>ijb,jkb-&gt;ikb</code>, taking diagonal part of a matrix is represented as <code>ii-&gt;i</code>. Sometimes, a single notation may have thousands or more tensors. Then the contraction order becomes relevant to computational cost of evaluating it.</p><h2 id="Contraction-order-and-complexity"><a class="docs-heading-anchor" href="#Contraction-order-and-complexity">Contraction order and complexity</a><a id="Contraction-order-and-complexity-1"></a><a class="docs-heading-anchor-permalink" href="#Contraction-order-and-complexity" title="Permalink"></a></h2><p><em>Contraction order</em> is a key factor for the performance of tensor network contraction. It is represented by a binary tree, where the leaves are the tensors to be contracted and the internal nodes are the intermediate tensors. The quality of the contraction order is quantified by the <em>contraction complexity</em>, which consists of the following metrics</p><ul><li><strong>time complexity</strong>: the number of floating point operations required to calculate the result;</li><li><strong>space complexity</strong>: the largest size of the intermediate tensors. For larger tensor networks, the contraction order is important, since it can greatly reduce the time complexity of the calculation.</li><li><strong>read-write complexity</strong>: the number of times the intermediate tensors are read and written.</li></ul><p>The contraction order optimization aims to find the <em>optimal contraction order</em> with the lowest cost, which is usually defined as some linear combination of the above complexities.</p><p>Finding the optimal contraction order is NP-complete, but fortunately, a close-to-optimal contraction order is usually good enough, which could be found in a reasonable time with a heuristic optimizer. In the past decade, methods have been developed to optimize the contraction orders, including both exact ones and heuristic ones. Among these methods, multiple heuristic methods can handle networks with more than <span>$10^4$</span> tensors efficiently <sup class="footnote-reference"><a id="citeref-Gray2021" href="#footnote-Gray2021">[Gray2021]</a></sup>, <sup class="footnote-reference"><a id="citeref-Roa2024" href="#footnote-Roa2024">[Roa2024]</a></sup>.</p><h3 id="Example:-Optimizing-the-contraction-order-with-OMEinsum.jl"><a class="docs-heading-anchor" href="#Example:-Optimizing-the-contraction-order-with-OMEinsum.jl">Example: Optimizing the contraction order with OMEinsum.jl</a><a id="Example:-Optimizing-the-contraction-order-with-OMEinsum.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Optimizing-the-contraction-order-with-OMEinsum.jl" title="Permalink"></a></h3><p>Considering the following simple tensor network:</p><pre><code class="language-julia hljs">julia&gt; einsum = ein&quot;ij, ik, jl, lk -&gt; &quot;
ij, ik, jl, lk -&gt;</code></pre><p><img src="../assets/ABCD.svg" alt/></p><p>Here we simply assume that all indices are of the same dimension <span>$D$</span>. Then the naive way to calculate the result is to loop over all the indices, which requires <span>$O(D^4)$</span> operations and no intermediate tensors are produced.</p><p>Alternatively, we can contract step by step. We first contraction tensors <span>$A, B \mapsto A B$</span>, and <span>$C, D \mapsto C D$</span>, which produces two rank-2 intermediate tensors, and then contract <span>$A B$</span> with <span>$C D$</span> to get the scalar <span>$s$</span>.</p><p><img src="../assets/ABCD_contraction.svg" alt/></p><p>It is equivalent to the following einsum notation:</p><pre><code class="language-julia hljs">julia&gt; nested_ein = ein&quot;(ij, ik), (jl, lk) -&gt; &quot;
jk, jk -&gt;
├─ ij, ik -&gt; jk
│  ├─ ij
│  └─ ik
└─ jl, lk -&gt; jk
   ├─ jl
   └─ lk</code></pre><p>In this way, the total number of operations is <span>$O(2 D^{3} + D^{2})$</span>, which is smaller than the naive calculation, while the trade-off is that we need to store the intermediate tensors <span>$AB$</span> and <span>$CD$</span> with size of <span>$O(D^{2})$</span>, as shown below:</p><pre><code class="language-julia hljs"># here we take D = 16
julia&gt; size_dict = uniformsize(einsum, 2^4)

julia&gt; contraction_complexity(einsum, size_dict)
Time complexity: 2^16.0
Space complexity: 2^0.0
Read-write complexity: 2^10.001408194392809

julia&gt; contraction_complexity(nested_ein, size_dict)
Time complexity: 2^13.044394119358454
Space complexity: 2^8.0
Read-write complexity: 2^11.000704269011246</code></pre><p>We say such a contraction is with <strong>time complexity</strong> of <span>$O(D^{3})$</span> and <strong>space complexity</strong> of <span>$O(D^{4})$</span>.</p><p>In actual calculation, we prefer binary contractions, i.e., contracting two tensors at a time, by converting these two tensors as matrices, so that we can make use of BLAS libraries to speed up the calculation. In this way, a given contraction order can be represented as a binary tree. The contraction tree can be represented as a rooted tree, where the leaves are the tensors to be contracted and the internal nodes are the intermediate tensors. The contraction tree corresponding to the above example is shown below:</p><p><img src="../assets/ABCD_tree.svg" alt/></p><p>Generally speaking, our target is to find a binary contraction order, with minimal time complexity or space complexity, which is called the <strong>optimal contraction order</strong>.</p><h2 id="Tree-decomposition-and-tree-width"><a class="docs-heading-anchor" href="#Tree-decomposition-and-tree-width">Tree decomposition and tree width</a><a id="Tree-decomposition-and-tree-width-1"></a><a class="docs-heading-anchor-permalink" href="#Tree-decomposition-and-tree-width" title="Permalink"></a></h2><p>Finding the optimal contraction order is related to another NP-complete problem: finding the <a href="https://en.wikipedia.org/wiki/Tree_decomposition"><em>tree decomposition</em></a> of a graph with minimal <a href="https://en.wikipedia.org/wiki/Treewidth"><em>treewidth</em></a> <sup class="footnote-reference"><a id="citeref-Markov2008" href="#footnote-Markov2008">[Markov2008]</a></sup>. Tree width is a graph characteristic that measures how similar a graph is to a tree, the lower the tree width, the more similar the graph is to a tree. The way to relate a graph to a tree is to find a tree decomposition of the graph, which is a tree whose nodes are subsets of the vertices of the graph, and the following conditions are satisfied:</p><ol><li>Each vertex of the graph is in at least one node of the tree.</li><li>For each edge of the graph, there is a node of the tree containing both vertices of the edge.</li><li>Bags containing the same vertex have to be connected in the tree.</li></ol><p>All the nodes of the tree are called <em>tree bags</em>, and intersection of two bags is called a <em>separator</em>. The width of a tree decomposition is the size of the largest bag minus one. Clearly, one graph can have multiple tree decomposition with different corresponding widths. The tree width of a graph is the minimal width of all such decompositions, and a particular decomposition (not necessarily unique) that realises this minimal width is called an optimal tree decomposition. Its width is the tree width of the graph, denoted as <span>$\text{tw}(G)$</span>.</p><p>Let <span>$G$</span> be the hypergraph topology of a tensor network, where a tensor is mapped to a vertex and an index is mapped to an edge. Two tensors are connected if they share a common index. Instead, if we treat each index as a vertex and each tensor as an edge, we can get its <em>line graph</em> <span>$L(G)$</span>. Ref.<sup class="footnote-reference"><a id="citeref-Markov2008" href="#footnote-Markov2008">[Markov2008]</a></sup> shows that the bottleneck time complexity of the contraction of a tensor network is <span>$O(2^{\text{tw}(L(G))})$</span>, where <span>$\text{tw}(L(G))$</span> is the treewidth of <span>$L(G)$</span>. Therefore, if we can find the tree decomposition of the tensor network with minimal treewidth, we can find the optimal contraction order of the tensor network.</p><h3 id="Example:-Line-graph-and-tree-decomposition"><a class="docs-heading-anchor" href="#Example:-Line-graph-and-tree-decomposition">Example: Line graph and tree decomposition</a><a id="Example:-Line-graph-and-tree-decomposition-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Line-graph-and-tree-decomposition" title="Permalink"></a></h3><p>Consider the einsum notation:</p><pre><code class="language-julia hljs">julia&gt; ein&quot;ABC,BFG,EGH,CDE-&gt;&quot;</code></pre><p>Its graph <span>$G$</span>, its line graph <span>$L(G)$</span> and its tree decomposition are shown in the following figure:</p><p><img src="../assets/treedecomposition.svg" alt="Fig.1"/></p><p>The tree decomposition in figure (c) is related to the contraction order of the tensor network in the following way:</p><ul><li>By defintion of tree decomposition, each edge of the line graph is contained in at least one tree bag. Hence, every tensor (represented as a hyperedge in <span>$L(G)$</span>) can fit into a tree bag.</li><li>We contract the tensor networks from the leaves to the root. Tensor network contraction requires the indices used in the future steps must be kept in the output tensor. This is automatically satisfied by the third requirement of tree decomposition: If two bags containing two tensors sharing the same index, then this index must appear in all bags between them, such that they can be connected.</li></ul><p>As a remark, the tree decomposition considered here handles with weighted vertices. Since we are considering a tensor network, dimension of the indices have to be considered.  Therefore, for each vertex of the line graph <span>$L(G)$</span>, we define its weight as <span>$\log_2(d)$</span>, where <span>$d$</span> is the dimension of the index. In this way, size of a tensor can be represented as the sum of weights of the vertices in <span>$L(G)$</span>.</p><h2 id="Reduce-space-complexity-by-slicing"><a class="docs-heading-anchor" href="#Reduce-space-complexity-by-slicing">Reduce space complexity by slicing</a><a id="Reduce-space-complexity-by-slicing-1"></a><a class="docs-heading-anchor-permalink" href="#Reduce-space-complexity-by-slicing" title="Permalink"></a></h2><p>Slicing is a technique to reduce the space complexity of the tensor network by looping over a subset of indices. This effectively reduces the size of the tensor network inside the loop, and the space complexity can potentially be reduced. For example, in the following figure, we slice the tensor network over the index <span>$i$</span>. The label <span>$i$</span> is removed from the tensor network, at the cost of contraction multiple tensor networks.</p><p><img src="../assets/slicing.svg" alt/></p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Gray2021"><a class="tag is-link" href="#citeref-Gray2021">Gray2021</a>Gray, Johnnie, and Stefanos Kourtis. &quot;Hyper-optimized tensor network contraction.&quot; Quantum 5 (2021): 410.</li><li class="footnote" id="footnote-Markov2008"><a class="tag is-link" href="#citeref-Markov2008">Markov2008</a>Markov, I.L., Shi, Y., 2008. Simulating Quantum Computation by Contracting Tensor Networks. SIAM J. Comput. 38, 963–981. https://doi.org/10.1137/050644756</li><li class="footnote" id="footnote-Roa2024"><a class="tag is-link" href="#citeref-Roa2024">Roa2024</a>Roa-Villescas, M., Gao, X., Stuijk, S., Corporaal, H., Liu, J.-G., 2024. Probabilistic Inference in the Era of Tensor Networks and Differential Programming. Phys. Rev. Research 6, 033261. https://doi.org/10.1103/PhysRevResearch.6.033261</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../tutorial/">Tutorial »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.13.0 on <span class="colophon-date" title="Sunday 22 June 2025 14:40">Sunday 22 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
