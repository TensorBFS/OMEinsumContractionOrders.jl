var documenterSearchIndex = {"docs":
[{"location":"background/#Tensor-Network-Contraction-Order-Optimization","page":"Background Knowledge","title":"Tensor Network Contraction Order Optimization","text":"","category":"section"},{"location":"background/#Tensor-network","page":"Background Knowledge","title":"Tensor network","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Tensor network is a powerful tool for modeling and simulating quantum many-body systems, probabilistic inference, combinatorial optimization, etc. It is a diagrammatic representation of tensor contractions. In this representation, a tensor is represented as a node, and an index is represented as a hyperedge (a hyperedge can connect to any number of nodes). For example, vectors, matrices and higher order tensors can be represented as:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: )","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"To understand the basic concepts of tensor network, we recommend the following references for readers with different background:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"For readers with physics background: https://tensornetwork.org/diagrams/\nFor readers want to get a formal definition: Chapter 2 of https://epubs.siam.org/doi/abs/10.1137/22M1501787","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"The defining operation of a tensor network is contraction, which is also named as sum-product operation. It can be viewed as the generalization of matrix multiplication to tensors. We illustrate tensor network contraction with the following example.","category":"page"},{"location":"background/#Example:-Trace-permutation-rule","page":"Background Knowledge","title":"Example: Trace permutation rule","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Let A B and C be three square matrices with the same size. The trace operation texttr(A B C) is defined as the sum of the products of the elements of the matrices:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"texttr(A B C) = sum_ijk A_ij B_ik C_jk","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"It can be represented as a tensor network diagram as follows:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: )","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"So, each tensor is represented as a node, and each index is represented as an edge. Tensors sharing the same index are connected by the edge. The contraction of the tensor network is the summation of the products of the elements of the tensors over all possible values of the indices.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"The contraction can happen in different orders. The famous trace permutation rule states that the trace is invariant under cyclic permutations of the matrices:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"texttr(A B C) = texttr(C A B) = texttr(B C A)","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Hence, we observe that tensor network contraction is associative and commutative, i.e., the order of evaluation does not change the result. In the tensor network diagram, the order of evaluation is neglected, so the trace permutation rule is trivially satisfied.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"By drawing diagrams, we prove the trace permutation rule. Isn't it cool?","category":"page"},{"location":"background/#Einsum-notation","page":"Background Knowledge","title":"Einsum notation","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"For simplicity, in the following sections, we will use the einsum notation in OMEinsum.jl to represent the tensor network contraction. It is almost the same as the numpy einsum notation. For example, the contraction of the tensor network in the previous trace permutation example can be represented as:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"julia> using OMEinsum\n\njulia> trace_perm = ein\"ij, jk, ki -> \"\nij, jk, ki ->","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"The string ij, jk, ki -> is the einsum notation, where labels of the input tensors and the output tensor are separated by ->, and labels of different input tensors are separated by ,. Here, ij, jk, ki are the labels of the input tensors of rank 2 (matrices), and the label of the output tensor is empty, representing a scalar.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Einsum notation is a powerful tool to represent linear operations, e.g. batched matrix multiplication is represented as ijb,jkb->ikb, taking diagonal part of a matrix is represented as ii->i. Sometimes, a single notation may have thousands or more tensors. Then the contraction order becomes relevant to computational cost of evaluating it.","category":"page"},{"location":"background/#Contraction-order-and-complexity","page":"Background Knowledge","title":"Contraction order and complexity","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Contraction order is a key factor for the performance of tensor network contraction. It is represented by a binary tree, where the leaves are the tensors to be contracted and the internal nodes are the intermediate tensors. The quality of the contraction order is quantified by the contraction complexity, which consists of the following metrics","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"time complexity: the number of floating point operations required to calculate the result;\nspace complexity: the largest size of the intermediate tensors. For larger tensor networks, the contraction order is important, since it can greatly reduce the time complexity of the calculation.\nread-write complexity: the number of times the intermediate tensors are read and written.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"The contraction order optimization aims to find the optimal contraction order with the lowest cost, which is usually defined as some linear combination of the above complexities.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Finding the optimal contraction order is NP-complete, but fortunately, a close-to-optimal contraction order is usually good enough, which could be found in a reasonable time with a heuristic optimizer. In the past decade, methods have been developed to optimize the contraction orders, including both exact ones and heuristic ones. Among these methods, multiple heuristic methods can handle networks with more than 10^4 tensors efficiently [Gray2021], [Roa2024].","category":"page"},{"location":"background/#Example:-Optimizing-the-contraction-order-with-OMEinsum.jl","page":"Background Knowledge","title":"Example: Optimizing the contraction order with OMEinsum.jl","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Considering the following simple tensor network:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"julia> einsum = ein\"ij, ik, jl, lk -> \"\nij, ik, jl, lk ->","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: )","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Here we simply assume that all indices are of the same dimension D. Then the naive way to calculate the result is to loop over all the indices, which requires O(D^4) operations and no intermediate tensors are produced.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Alternatively, we can contract step by step. We first contraction tensors A B mapsto A B, and C D mapsto C D, which produces two rank-2 intermediate tensors, and then contract A B with C D to get the scalar s.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: )","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"It is equivalent to the following einsum notation:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"julia> nested_ein = ein\"(ij, ik), (jl, lk) -> \"\njk, jk ->\n├─ ij, ik -> jk\n│  ├─ ij\n│  └─ ik\n└─ jl, lk -> jk\n   ├─ jl\n   └─ lk","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"In this way, the total number of operations is O(2 D^3 + D^2), which is smaller than the naive calculation, while the trade-off is that we need to store the intermediate tensors AB and CD with size of O(D^2), as shown below:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"# here we take D = 16\njulia> size_dict = uniformsize(einsum, 2^4)\n\njulia> contraction_complexity(einsum, size_dict)\nTime complexity: 2^16.0\nSpace complexity: 2^0.0\nRead-write complexity: 2^10.001408194392809\n\njulia> contraction_complexity(nested_ein, size_dict)\nTime complexity: 2^13.044394119358454\nSpace complexity: 2^8.0\nRead-write complexity: 2^11.000704269011246","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"We say such a contraction is with time complexity of O(D^3) and space complexity of O(D^4).","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"In actual calculation, we prefer binary contractions, i.e., contracting two tensors at a time, by converting these two tensors as matrices, so that we can make use of BLAS libraries to speed up the calculation. In this way, a given contraction order can be represented as a binary tree. The contraction tree can be represented as a rooted tree, where the leaves are the tensors to be contracted and the internal nodes are the intermediate tensors. The contraction tree corresponding to the above example is shown below:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: )","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Generally speaking, our target is to find a binary contraction order, with minimal time complexity or space complexity, which is called the optimal contraction order.","category":"page"},{"location":"background/#Tree-decomposition-and-tree-width","page":"Background Knowledge","title":"Tree decomposition and tree width","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Finding the optimal contraction order is related to another NP-complete problem: finding the tree decomposition of a graph with minimal treewidth [Markov2008]. Tree width is a graph characteristic that measures how similar a graph is to a tree, the lower the tree width, the more similar the graph is to a tree. The way to relate a graph to a tree is to find a tree decomposition of the graph, which is a tree whose nodes are subsets of the vertices of the graph, and the following conditions are satisfied:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Each vertex of the graph is in at least one node of the tree.\nFor each edge of the graph, there is a node of the tree containing both vertices of the edge.\nBags containing the same vertex have to be connected in the tree.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"All the nodes of the tree are called tree bags, and intersection of two bags is called a separator. The width of a tree decomposition is the size of the largest bag minus one. Clearly, one graph can have multiple tree decomposition with different corresponding widths. The tree width of a graph is the minimal width of all such decompositions, and a particular decomposition (not necessarily unique) that realises this minimal width is called an optimal tree decomposition. Its width is the tree width of the graph, denoted as texttw(G).","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Let G be the hypergraph topology of a tensor network, where a tensor is mapped to a vertex and an index is mapped to an edge. Two tensors are connected if they share a common index. Instead, if we treat each index as a vertex and each tensor as an edge, we can get its line graph L(G). Ref.[Markov2008] shows that the bottleneck time complexity of the contraction of a tensor network is O(2^texttw(L(G))), where texttw(L(G)) is the treewidth of L(G). Therefore, if we can find the tree decomposition of the tensor network with minimal treewidth, we can find the optimal contraction order of the tensor network.","category":"page"},{"location":"background/#Example:-Line-graph-and-tree-decomposition","page":"Background Knowledge","title":"Example: Line graph and tree decomposition","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Consider the einsum notation:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"julia> ein\"ABC,BFG,EGH,CDE->\"","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Its graph G, its line graph L(G) and its tree decomposition are shown in the following figure:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: Fig.1)","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"The tree decomposition in figure (c) is related to the contraction order of the tensor network in the following way:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"By defintion of tree decomposition, each edge of the line graph is contained in at least one tree bag. Hence, every tensor (represented as a hyperedge in L(G)) can fit into a tree bag.\nWe contract the tensor networks from the leaves to the root. Tensor network contraction requires the indices used in the future steps must be kept in the output tensor. This is automatically satisfied by the third requirement of tree decomposition: If two bags containing two tensors sharing the same index, then this index must appear in all bags between them, such that they can be connected.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"As a remark, the tree decomposition considered here handles with weighted vertices. Since we are considering a tensor network, dimension of the indices have to be considered.  Therefore, for each vertex of the line graph L(G), we define its weight as log_2(d), where d is the dimension of the index. In this way, size of a tensor can be represented as the sum of weights of the vertices in L(G).","category":"page"},{"location":"background/#Reduce-space-complexity-by-slicing","page":"Background Knowledge","title":"Reduce space complexity by slicing","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Slicing is a technique to reduce the space complexity of the tensor network by looping over a subset of indices. This effectively reduces the size of the tensor network inside the loop, and the space complexity can potentially be reduced. For example, in the following figure, we slice the tensor network over the index i. The label i is removed from the tensor network, at the cost of contraction multiple tensor networks.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: )","category":"page"},{"location":"background/#References","page":"Background Knowledge","title":"References","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"[Gray2021]: Gray, Johnnie, and Stefanos Kourtis. \"Hyper-optimized tensor network contraction.\" Quantum 5 (2021): 410.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"[Markov2008]: Markov, I.L., Shi, Y., 2008. Simulating Quantum Computation by Contracting Tensor Networks. SIAM J. Comput. 38, 963–981. https://doi.org/10.1137/050644756","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"[Roa2024]: Roa-Villescas, M., Gao, X., Stuijk, S., Corporaal, H., Liu, J.-G., 2024. Probabilistic Inference in the Era of Tensor Networks and Differential Programming. Phys. Rev. Research 6, 033261. https://doi.org/10.1103/PhysRevResearch.6.033261","category":"page"},{"location":"ref/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"ref/#Data-structures-and-interfaces","page":"Reference","title":"Data structures and interfaces","text":"","category":"section"},{"location":"ref/#OMEinsumContractionOrders.AbstractEinsum","page":"Reference","title":"OMEinsumContractionOrders.AbstractEinsum","text":"AbstractEinsum\n\nAbstract type for einsum notations.\n\nRequired Interfaces\n\ngetixsv: a vector of vectors, each vector represents the labels associated with a input tensor.\ngetiyv: a vector of labels associated with the output tensor.\nuniquelabels: a vector of labels that are unique in the einsum notation.\n\nDerived interfaces\n\nlabeltype: the data type to represent the labels in the einsum notation.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.CodeOptimizer","page":"Reference","title":"OMEinsumContractionOrders.CodeOptimizer","text":"CodeOptimizer\n\nAbstract type for code optimizers.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.EinCode","page":"Reference","title":"OMEinsumContractionOrders.EinCode","text":"EinCode{LT} <: AbstractEinsum\nEinCode(ixs::Vector{Vector{LT}}, iy::Vector{LT})\n\nEinsum code with input indices ixs and output index iy.\n\nExamples\n\nThe einsum notation for matrix multiplication is:\n\njulia> code = OMEinsumContractionOrders.EinCode([[1,2], [2, 3]], [1, 3])\n1∘2, 2∘3 -> 1∘3\n\njulia> OMEinsumContractionOrders.getixsv(code)\n2-element Vector{Vector{Int64}}:\n [1, 2]\n [2, 3]\n\njulia> OMEinsumContractionOrders.getiyv(code)\n2-element Vector{Int64}:\n 1\n 3\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.NestedEinsum","page":"Reference","title":"OMEinsumContractionOrders.NestedEinsum","text":"NestedEinsum{LT} <: AbstractEinsum\nNestedEinsum(args::Vector{NestedEinsum}, eins::EinCode)\n\nThe einsum notation with a contraction order specified as a tree data structure. It is automatically generated by the contraction code optimizer with the optimize_code function.\n\nFields\n\nargs: the children of the current node\ntensorindex: the index of the input tensor, required only for leaf nodes. For non-leaf nodes, it is -1.\neins: the einsum notation for the operation at the current node.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.SlicedEinsum","page":"Reference","title":"OMEinsumContractionOrders.SlicedEinsum","text":"SlicedEinsum{LT,ET<:Union{EinCode{LT},NestedEinsum{LT}}} <: AbstractEinsum\nSlicedEinsum(slicing::Vector{LT}, eins::ET)\n\nThe einsum notation with sliced indices. The sliced indices are the indices enumerated manually at the top level. By slicing the indices, the space complexity of the einsum notation can be reduced.\n\nFields\n\nslicing: the sliced indices.\neins: the einsum notation of the current node, which is a NestedEinsum object.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.getixsv","page":"Reference","title":"OMEinsumContractionOrders.getixsv","text":"getixsv(code::AbstractEinsum) -> Vector{Vector{LT}}\n\nReturns the input indices of the einsum notation. Each vector represents the labels associated with a input tensor.\n\n\n\n\n\n","category":"function"},{"location":"ref/#OMEinsumContractionOrders.getiyv","page":"Reference","title":"OMEinsumContractionOrders.getiyv","text":"getiyv(code::AbstractEinsum) -> Vector{LT}\n\nReturns the output index of the einsum notation.\n\n\n\n\n\n","category":"function"},{"location":"ref/#OMEinsumContractionOrders.labeltype-Tuple{OMEinsumContractionOrders.AbstractEinsum}","page":"Reference","title":"OMEinsumContractionOrders.labeltype","text":"labeltype(code::AbstractEinsum) -> Type\n\nReturns the data type to represent the labels in the einsum notation.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.uniquelabels-Tuple{OMEinsumContractionOrders.AbstractEinsum}","page":"Reference","title":"OMEinsumContractionOrders.uniquelabels","text":"uniquelabels(code::AbstractEinsum) -> Vector{LT}\n\nReturns the unique labels in the einsum notation. The labels are the indices of the tensors.\n\n\n\n\n\n","category":"method"},{"location":"ref/#Time-and-space-complexity","page":"Reference","title":"Time and space complexity","text":"","category":"section"},{"location":"ref/#OMEinsumContractionOrders.contraction_complexity-Tuple{OMEinsumContractionOrders.AbstractEinsum, Any}","page":"Reference","title":"OMEinsumContractionOrders.contraction_complexity","text":"contraction_complexity(eincode, size_dict) -> ContractionComplexity\n\nReturns the time, space and read-write complexity of the einsum contraction. The returned ContractionComplexity object contains 3 fields:\n\ntc: time complexity defined as log2(number of element-wise multiplications).\nsc: space complexity defined as log2(size of the maximum intermediate tensor).\nrwc: read-write complexity defined as log2(the number of read-write operations).\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.flop-Union{Tuple{VT}, Tuple{LT}, Tuple{OMEinsumContractionOrders.EinCode, Dict{LT, VT}}} where {LT, VT}","page":"Reference","title":"OMEinsumContractionOrders.flop","text":"flop(eincode, size_dict) -> Int\n\nReturns the number of iterations, which is different with the true floating point operations (FLOP) by a factor of 2.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.label_elimination_order-Tuple{OMEinsumContractionOrders.NestedEinsum}","page":"Reference","title":"OMEinsumContractionOrders.label_elimination_order","text":"label_elimination_order(code) -> Vector\n\nReturns a vector of labels sorted by the order they are eliminated in the contraction tree. The contraction tree is specified by code, which e.g. can be a NestedEinsum instance.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.peak_memory-Tuple{OMEinsumContractionOrders.NestedEinsum, Dict}","page":"Reference","title":"OMEinsumContractionOrders.peak_memory","text":"peak_memory(code, size_dict::Dict) -> Int\n\nEstimate peak memory in number of elements.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.uniformsize-Tuple{OMEinsumContractionOrders.AbstractEinsum, Any}","page":"Reference","title":"OMEinsumContractionOrders.uniformsize","text":"uniformsize(code::AbstractEinsum, size::Int) -> Dict\n\nReturns a dictionary that maps each label to the given size.\n\n\n\n\n\n","category":"method"},{"location":"ref/#Contraction-order-optimizers","page":"Reference","title":"Contraction order optimizers","text":"","category":"section"},{"location":"ref/#OMEinsumContractionOrders.optimize_code","page":"Reference","title":"OMEinsumContractionOrders.optimize_code","text":"optimize_code(eincode, size_dict, optimizer = GreedyMethod(), simplifier=nothing, permute=true) -> optimized_eincode\n\nOptimize the einsum contraction code and reduce the time/space complexity of tensor network contraction. Returns a NestedEinsum instance. Input arguments are\n\nArguments\n\neincode is an einsum contraction code instance, one of DynamicEinCode, StaticEinCode or NestedEinsum.\nsize is a dictionary of \"edge label=>edge size\" that contains the size information, one can use uniformsize(eincode, 2) to create a uniform size.\noptimizer is a CodeOptimizer instance, should be one of GreedyMethod, Treewidth, KaHyParBipartite, SABipartite or TreeSA. Check their docstrings for details.\nsimplifier is one of MergeVectors or MergeGreedy.\npermute is a boolean flag to indicate whether to optimize the permutation of the contraction order.\n\nExamples\n\njulia> using OMEinsum\n\njulia> code = ein\"ij, jk, kl, il->\"\nij, jk, kl, il -> \n\njulia> optimize_code(code, uniformsize(code, 2), TreeSA());\n\n\n\n\n\n","category":"function"},{"location":"ref/#OMEinsumContractionOrders.GreedyMethod","page":"Reference","title":"OMEinsumContractionOrders.GreedyMethod","text":"GreedyMethod{MT}\nGreedyMethod(; α = 0.0, temperature = 0.0, nrepeat=1)\n\nThe fast but poor greedy optimizer.\n\nFields\n\nα is the parameter for the loss function, for pairwise interaction, L = size(out) - α * (size(in1) + size(in2))\ntemperature is the parameter for sampling, if it is zero, the minimum loss is selected; for non-zero, the loss is selected by the Boltzmann distribution, given by p ~ exp(-loss/temperature).\nnrepeat is the number of repeatition, returns the best contraction order.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.optimize_greedy-Union{Tuple{T2}, Tuple{TT}, Tuple{TA}, Tuple{L}, Tuple{OMEinsumContractionOrders.EinCode{L}, Dict{L, T2}}} where {L, TA, TT, T2}","page":"Reference","title":"OMEinsumContractionOrders.optimize_greedy","text":"optimize_greedy(eincode, size_dict; α = 0.0, temperature = 0.0, nrepeat=1)\n\nGreedy optimizing the contraction order and return a NestedEinsum object. Check the docstring of tree_greedy for detailed explaination of other input arguments.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.tree_greedy-Union{Tuple{ET}, Tuple{TT}, Tuple{TA}, Tuple{OMEinsumContractionOrders.IncidenceList{Int64, ET}, Any}} where {TA, TT, ET}","page":"Reference","title":"OMEinsumContractionOrders.tree_greedy","text":"tree_greedy(incidence_list, log2_sizes; α = 0.0, temperature = 0.0, nrepeat=1)\n\nCompute greedy order, and the time and space complexities, the rows of the incidence_list are vertices and columns are edges. log2_sizes are defined on edges. α is the parameter for the loss function, for pairwise interaction, L = size(out) - α * (size(in1) + size(in2)) temperature is the parameter for sampling, if it is zero, the minimum loss is selected; for non-zero, the loss is selected by the Boltzmann distribution, given by p ~ exp(-loss/temperature).\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.TreeSA","page":"Reference","title":"OMEinsumContractionOrders.TreeSA","text":"TreeSA{RT,IT,GM,LT} <: CodeOptimizer\nTreeSA(; sc_target=20, βs=collect(0.01:0.05:15), ntrials=10, niters=50,\n    sc_weight=1.0, rw_weight=0.2, initializer=:greedy, greedy_config=GreedyMethod(; nrepeat=1))\n\nOptimize the einsum contraction pattern using the simulated annealing on tensor expression tree.\n\nFields\n\nsc_target is the target space complexity,\nntrials, βs and niters are annealing parameters, doing ntrials indepedent annealings, each has inverse tempteratures specified by βs, in each temperature, do niters updates of the tree.\nsc_weight is the relative importance factor of space complexity in the loss compared with the time complexity.\nrw_weight is the relative importance factor of memory read and write in the loss compared with the time complexity.\ninitializer specifies how to determine the initial configuration, it can be :greedy or :random. If it is using :greedy method to generate the initial configuration, it also uses two extra arguments greedy_method and greedy_nrepeat.\nnslices is the number of sliced legs, default is 0.\nfixed_slices is a vector of sliced legs, default is [].\ngreedy_config is the configuration for the greedy method used for initializing the tree.\n\nReferences\n\nRecursive Multi-Tensor Contraction for XEB Verification of Quantum Circuits\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.optimize_tree-Tuple{OMEinsumContractionOrders.AbstractEinsum, Any}","page":"Reference","title":"OMEinsumContractionOrders.optimize_tree","text":"optimize_tree(code, size_dict; sc_target=20, βs=0.1:0.1:10, ntrials=2, niters=100, sc_weight=1.0, rw_weight=0.2, initializer=:greedy, greedy_method=MinSpaceOut(), fixed_slices=[])\n\nOptimize the einsum contraction pattern specified by code, and edge sizes specified by size_dict. Check the docstring of TreeSA for detailed explaination of other input arguments.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.ExactTreewidth","page":"Reference","title":"OMEinsumContractionOrders.ExactTreewidth","text":"const ExactTreewidth{GM} = Treewidth{SafeRules{BT, MMW{3}(), MF}, GM}\nExactTreewidth(; greedy_config = GreedyMethod(nrepeat=1)) = Treewidth(; greedy_config)\n\nExactTreewidth is a specialization of Treewidth for the SafeRules preprocessing algorithm with the BT elimination algorithm. The BT algorithm is an exact solver for the treewidth problem that implemented in TreeWidthSolver.jl.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.Treewidth","page":"Reference","title":"OMEinsumContractionOrders.Treewidth","text":"struct Treewidth{EL <: EliminationAlgorithm, GM} <: CodeOptimizer\nTreewidth(; alg::EL = SafeRules(BT(), MMW{3}(), MF()), greedy_config::GM = GreedyMethod(nrepeat=1))\n\nTree width based solver. The solvers are implemented in CliqueTrees.jl and TreeWidthSolver.jl. They include:\n\nAlgorithm Description Time Complexity Space Complexity\nBFS breadth-first search O(m + n) O(n)\nMCS maximum cardinality search O(m + n) O(n)\nLexBFS lexicographic breadth-first search O(m + n) O(m + n)\nRCMMD reverse Cuthill-Mckee (minimum degree) O(m + n) O(m + n)\nRCMGL reverse Cuthill-Mckee (George-Liu) O(m + n) O(m + n)\nMCSM maximum cardinality search (minimal) O(mn) O(n)\nLexM lexicographic breadth-first search (minimal) O(mn) O(n)\nAMF approximate minimum fill O(mn) O(m + n)\nMF minimum fill O(mn²) -\nMMD multiple minimum degree O(mn²) O(m + n)\n\nDetailed descriptions is available in the CliqueTrees.jl.\n\nFields\n\nalg::EL: The algorithm to use for the treewidth calculation. Available elimination algorithms are listed above.\ngreedy_config::GM: The configuration for the greedy method.\n\nExample\n\njulia> optimizer = Treewidth();\n\njulia> eincode = OMEinsumContractionOrders.EinCode([['a', 'b'], ['a', 'c', 'd'], ['b', 'c', 'e', 'f'], ['e'], ['d', 'f']], ['a'])\nab, acd, bcef, e, df -> a\n\njulia> size_dict = Dict([c=>(1<<i) for (i,c) in enumerate(['a', 'b', 'c', 'd', 'e', 'f'])]...)\nDict{Char, Int64} with 6 entries:\n  'f' => 64\n  'a' => 2\n  'c' => 8\n  'd' => 16\n  'e' => 32\n  'b' => 4\n\njulia> optcode = optimize_code(eincode, size_dict, optimizer)\nab, ab -> a\n├─ fac, bcf -> ab\n│  ├─ df, acd -> fac\n│  │  ├─ df\n│  │  └─ acd\n│  └─ e, bcef -> bcf\n│     ├─ e\n│     └─ bcef\n└─ ab\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.optimize_treewidth-Union{Tuple{EL}, Tuple{GM}, Tuple{Treewidth{EL, GM}, OMEinsumContractionOrders.AbstractEinsum, Dict}} where {GM, EL}","page":"Reference","title":"OMEinsumContractionOrders.optimize_treewidth","text":"optimize_treewidth(optimizer, eincode, size_dict)\n\nOptimizing the contraction order via solve the exact tree width of the line graph corresponding to the eincode and return a NestedEinsum object. Check the docstring of treewidth_method for detailed explaination of other input arguments.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.HyperND","page":"Reference","title":"OMEinsumContractionOrders.HyperND","text":"HyperND(;\n    dis = KaHyParND(),\n    algs = (MF(), MMD()),\n    level = 6,\n    width = 120,\n    imbalances = 130:130,\n)\n\nNested-dissection based optimizer. Recursively partitions a tensor network, then calls a greedy algorithm on the leaves. The optimizer is run a number of times: once for each greedy algorithm in algs and each imbalance value in imbalances. The recursion depth is controlled by the parameters level and width.\n\nThe line graph is partitioned using the algorithm dis. OMEinsumContractionOrders currently supports two partitioning algorithms, both of which require importing an external library.\n\ntype package\nMETISND Metis.jl\nKaHyParND KayHyPar.jl\n\nThe optimizer is implemented using the tree decomposition library CliqueTrees.jl.\n\nArguments\n\ndis: graph partitioning algorithm\nalgs: tuple of elimination algorithms.\nlevel: maximum level\nwidth: minimum width\nimbalances: imbalance parameters \n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.KaHyParBipartite","page":"Reference","title":"OMEinsumContractionOrders.KaHyParBipartite","text":"KaHyParBipartite{RT,IT,GM}\nKaHyParBipartite(; sc_target, imbalances=collect(0.0:0.005:0.8),\n    max_group_size=40, greedy_config=GreedyMethod())\n\nOptimize the einsum code contraction order using the KaHyPar + Greedy approach. This program first recursively cuts the tensors into several groups using KaHyPar, with maximum group size specifed by max_group_size and maximum space complexity specified by sc_target, Then finds the contraction order inside each group with the greedy search algorithm. Other arguments are\n\nFields\n\nsc_target is the target space complexity, defined as log2(number of elements in the largest tensor),\nimbalances is a KaHyPar parameter that controls the group sizes in hierarchical bipartition,\nmax_group_size is the maximum size that allowed to used greedy search,\nsub_optimizer is the sub-optimizer used to find the contraction order when the group size is small enough.\n\nReferences\n\nHyper-optimized tensor network contraction\nSimulating the Sycamore quantum supremacy circuits\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.optimize_kahypar-Tuple{OMEinsumContractionOrders.EinCode, Any}","page":"Reference","title":"OMEinsumContractionOrders.optimize_kahypar","text":"optimize_kahypar(code, size_dict; sc_target, max_group_size=40, imbalances=0.0:0.01:0.2, greedy_method=MinSpaceOut(), greedy_nrepeat=1)\n\nOptimize the einsum code contraction order using the KaHyPar + Greedy approach. size_dict is a dictionary that specifies leg dimensions.  Check the docstring of KaHyParBipartite for detailed explaination of other input arguments.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.optimize_kahypar_auto-Tuple{OMEinsumContractionOrders.EinCode, Any}","page":"Reference","title":"OMEinsumContractionOrders.optimize_kahypar_auto","text":"optimize_kahypar_auto(code, size_dict; max_group_size=40, sub_optimizer = GreedyMethod())\n\nFind the optimal contraction order automatically by determining the sc_target with bisection. It can fail if the tree width of your graph is larger than 100.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.SABipartite","page":"Reference","title":"OMEinsumContractionOrders.SABipartite","text":"SABipartite{RT,BT}\nSABipartite(; sc_target=25, ntrials=50, βs=0.1:0.2:15.0, niters=1000\n    max_group_size=40, greedy_config=GreedyMethod(), initializer=:random)\n\nOptimize the einsum code contraction order using the Simulated Annealing bipartition + Greedy approach. This program first recursively cuts the tensors into several groups using simulated annealing, with maximum group size specifed by max_group_size and maximum space complexity specified by sc_target, Then finds the contraction order inside each group with the greedy search algorithm. Other arguments are\n\nFields\n\nsc_target is the target space complexity, defined as log2(number of elements in the largest tensor),\nntrials is the number of repetition (with different random seeds),\nβs is a list of inverse temperature 1/T,\nniters is the number of iteration in each temperature,\nmax_group_size is the maximum size that allowed to used greedy search,\nsub_optimizer is the optimizer for the bipartited sub graphs, one can choose GreedyMethod() or TreeSA(),\ninitializer is the partition configuration initializer, one can choose :random or :greedy (slow but better).\n\nReferences\n\nHyper-optimized tensor network contraction\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.optimize_sa-Tuple{OMEinsumContractionOrders.EinCode, Any}","page":"Reference","title":"OMEinsumContractionOrders.optimize_sa","text":"optimize_sa(code, size_dict; sc_target, max_group_size=40, βs=0.1:0.2:15.0, niters=1000, ntrials=50,\n       sub_optimizer = GreedyMethod(), initializer=:random)\n\nOptimize the einsum code contraction order using the Simulated Annealing bipartition + Greedy approach. size_dict is a dictionary that specifies leg dimensions.  Check the docstring of SABipartite for detailed explaination of other input arguments.\n\nReferences\n\nHyper-optimized tensor network contraction\n\n\n\n\n\n","category":"method"},{"location":"ref/#Preprocessing-code","page":"Reference","title":"Preprocessing code","text":"","category":"section"},{"location":"ref/","page":"Reference","title":"Reference","text":"Some optimizers (e.g. TreeSA) are too costly to run. We provide a preprocessing step to reduce the time of calling optimizers.","category":"page"},{"location":"ref/#OMEinsumContractionOrders.CodeSimplifier","page":"Reference","title":"OMEinsumContractionOrders.CodeSimplifier","text":"CodeSimplifier\n\nAbstract type for code simplifiers.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.MergeGreedy","page":"Reference","title":"OMEinsumContractionOrders.MergeGreedy","text":"MergeGreedy <: CodeSimplifier\nMergeGreedy(; threshhold=-1e-12)\n\nContraction code simplifier (in order to reduce the time of calling optimizers) that merges tensors greedily if the space complexity of merged tensors is reduced (difference smaller than the threshhold).\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.MergeVectors","page":"Reference","title":"OMEinsumContractionOrders.MergeVectors","text":"MergeVectors <: CodeSimplifier\nMergeVectors()\n\nContraction code simplifier (in order to reduce the time of calling optimizers) that merges vectors to closest tensors.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.NetworkSimplifier","page":"Reference","title":"OMEinsumContractionOrders.NetworkSimplifier","text":"NetworkSimplifier{LT}\n\nA network simplifier that contains a list of operations that can be applied to a tensor network to reduce the number of tensors. It is generated from a proprocessor, such as MergeVectors or MergeGreedy.\n\nFields\n\noperations: a list of NestedEinsum objects.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.embed_simplifier-Tuple{OMEinsumContractionOrders.NestedEinsum, OMEinsumContractionOrders.NetworkSimplifier}","page":"Reference","title":"OMEinsumContractionOrders.embed_simplifier","text":"embed_simplifier(code::NestedEinsum, simplifier::NetworkSimplifier)\n\nEmbed the simplifier into the contraction code. A typical workflow is: (i) generate a simplifier with simplify_code, (ii) then optimize the simplified code with optimize_code and (iii) post-process the optimized code with embed_simplifier to produce correct contraction order for the original code. This is automatically done in optimize_code given the simplifier argument is not nothing.\n\nArguments\n\ncode: the contraction code to embed the simplifier into.\nsimplifier: the simplifier to embed, which is a NetworkSimplifier object.\n\nReturns\n\nA new NestedEinsum object.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.simplify_code-Tuple{Union{OMEinsumContractionOrders.EinCode, OMEinsumContractionOrders.NestedEinsum}, Any, MergeVectors}","page":"Reference","title":"OMEinsumContractionOrders.simplify_code","text":"simplify_code(code::Union{EinCode, NestedEinsum}, size_dict, method::CodeSimplifier)\n\nSimplify the contraction code by preprocessing the code with a simplifier.\n\nArguments\n\ncode: the contraction code to simplify.\nsize_dict: the size dictionary of the contraction code.\nmethod: the simplifier to use, which can be MergeVectors or MergeGreedy.\n\nReturns\n\nA tuple of (NetworkSimplifier, newcode), where newcode is a new EinCode object.\n\n\n\n\n\n","category":"method"},{"location":"ref/#Dump-and-load-contraction-orders","page":"Reference","title":"Dump and load contraction orders","text":"","category":"section"},{"location":"ref/#OMEinsumContractionOrders.readjson-Tuple{AbstractString}","page":"Reference","title":"OMEinsumContractionOrders.readjson","text":"readjson(filename::AbstractString)\n\nRead the contraction order from a JSON file.\n\nArguments\n\nfilename: the name of the file to read from.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.writejson-Tuple{AbstractString, Union{OMEinsumContractionOrders.NestedEinsum, OMEinsumContractionOrders.SlicedEinsum}}","page":"Reference","title":"OMEinsumContractionOrders.writejson","text":"writejson(filename::AbstractString, ne::Union{NestedEinsum, SlicedEinsum})\n\nWrite the contraction order to a JSON file.\n\nArguments\n\nfilename: the name of the file to write to.\nne: the contraction order to write. It can be a NestedEinsum or a SlicedEinsum object.\n\n\n\n\n\n","category":"method"},{"location":"ref/#Visualization","page":"Reference","title":"Visualization","text":"","category":"section"},{"location":"ref/","page":"Reference","title":"Reference","text":"Requires using LuxorGraphPlot to load the extension.","category":"page"},{"location":"ref/#OMEinsumContractionOrders.viz_contraction-Tuple","page":"Reference","title":"OMEinsumContractionOrders.viz_contraction","text":"viz_contraction(code::Union{NestedEinsum, SlicedEinsum}; locs=StressLayout(), framerate=10, filename=tempname() * \".mp4\", show_progress=true)\n\nVisualize the contraction process of a tensor network.\n\nArguments\n\ncode: The tensor network to visualize.\n\nKeyword Arguments\n\nlocs: The coordinates or layout algorithm to use for positioning the nodes in the graph. Default is StressLayout().\nframerate: The frame rate of the animation. Default is 10.\nfilename: The name of the output file, with .gif or .mp4 extension. Default is a temporary file with .mp4 extension.\nshow_progress: Whether to show progress information. Default is true.\n\nReturns\n\nthe path of the generated file.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.viz_eins-Tuple","page":"Reference","title":"OMEinsumContractionOrders.viz_eins","text":"viz_eins(code::AbstractEinsum; locs=StressLayout(), filename = nothing, kwargs...)\n\nVisualizes an AbstractEinsum object by creating a tensor network graph and rendering it using GraphViz.\n\nArguments\n\ncode::AbstractEinsum: The AbstractEinsum object to visualize.\n\nKeyword Arguments\n\nlocs=StressLayout(): The coordinates or layout algorithm to use for positioning the nodes in the graph.\nfilename = nothing: The name of the file to save the visualization to. If nothing, the visualization will be displayed on the screen instead of saving to a file.\nconfig = GraphDisplayConfig(): The configuration for displaying the graph. Please refer to the documentation of GraphDisplayConfig for more information.\nkwargs...: Additional keyword arguments to be passed to the GraphViz constructor.\n\n\n\n\n\n","category":"method"},{"location":"#OMEinsumContractionOrders","page":"Home","title":"OMEinsumContractionOrders","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is the documentation for OMEinsumContractionOrders, a Julia package for the optimization of the contraction order of tensor networks.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Installation guide is available in README.md. You can also access its features in OMEinsum, which uses it as the default contraction order optimizer.","category":"page"},{"location":"#Related-Packages","page":"Home","title":"Related Packages","text":"","category":"section"},{"location":"#Derived-packages","page":"Home","title":"Derived packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"OMEinsum.jl: a Julia package for tensor network contraction.\nTensorInference.jl: a Julia package for exact probabilistic inference based on the tensor network representation.\nYao.jl: a Julia package for quantum computing. Its tensor network based simulation backend use OMEinsumContractionOrders.jl as the default contraction order optimizer.\nGenericTensorNetworks.jl: a Julia package for computing solution space properties of computational hard problems. It is based on the generic tensor network method and its contraction order optimization backend is OMEinsumContractionOrders.jl.\nITensorNetworks.jl: a Julia package for physics simulation, which uses OMEinsumContractionOrders.jl as alternative contraction order optimization backend.","category":"page"},{"location":"#Similar-packages","page":"Home","title":"Similar packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Cotengra : a python library for contracting tensor networks or einsum expressions involving large numbers of tensors.","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#Example-1:-Optimize-contraction-order-without-backend-specified","page":"Tutorial","title":"Example 1: Optimize contraction order without backend specified","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"OMEinsumContractionOrders can be used as a standalone package to optimize the contraction order of an einsum notation. The first step is to construct an OMEinsumContractionOrders.EinCode object, which is a data type to represent the einsum notation.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using OMEinsumContractionOrders, Graphs, KaHyPar\nfunction random_regular_eincode(n, k; optimize=nothing)\n    g = Graphs.random_regular_graph(n, k)\n    ixs = [[minmax(e.src,e.dst)...] for e in Graphs.edges(g)]  # input indices\n    iy = Int[]  # output indices (scalar output)\n    return OMEinsumContractionOrders.EinCode(ixs, iy)\nend\ncode = random_regular_eincode(100, 3);","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here, we define an einsum notation with 3-regular graph topology. The vertices correspond to indices. On each edge, we specify a rank 2 tensor that associates with the vertices it connects. The output is a scalar.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"One can use contraction_complexity function to get the time, space and rewrite cost for contracting this einsum notation. This function takes two arguments: the einsum notation and a dictionary to specify the size of the variables. Here, we use the uniformsize function to specify that all variables have the same size 2. This function returns a dictionary that maps each variable to 2: Dict(i=>2 for i in uniquelabels(code)), where OMEinsumContractionOrders.uniquelabels returns the unique labels in the einsum notation.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"size_dict = uniformsize(code, 2)\ncontraction_complexity(code, size_dict)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Since we do not specify a contraction order, the direct contraction corresponds to brute force enumeration and costs 2^textnumber of vertices operations. No space is required to store the intermediate contraction result and the space complexity is 0. The read-write complexity corresponds to how many element-wise read and write operations are required to perform contraction.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The order of contraction is optimized by the optimize_code function. It takes three arguments: code, size_dict, and optimizer. The optimizer argument is the optimizer to be used. The available optimizers are listed in the optimizers page.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"optcode_tree = optimize_code(code, uniformsize(code, 2),\n\tTreeSA(sc_target=28, βs=0.1:0.1:10, ntrials=2, niters=20, sc_weight=3.0, nslices=5));\ncontraction_complexity(optcode_tree, size_dict)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The optimize_code function returns the optimized contraction order. The optimized contraction order is a OMEinsumContractionOrders.NestedEinsum object, which is a data type to represent the nested einsum notation.","category":"page"},{"location":"tutorial/#Example-2:-Use-it-in-OMEinsum","page":"Tutorial","title":"Example 2: Use it in OMEinsum","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"OMEinsumContractionOrders is shipped with OMEinsum package, which is a powerful package for tensor network contraction (or einsum contraction). You can use it to optimize the contraction order of an OMEinsum notation.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using OMEinsum\n\ncode = ein\"ij, jk, kl, il->\"\n\noptimized_code = optimize_code(code, uniformsize(code, 2), TreeSA())","category":"page"},{"location":"tutorial/#Example-3:-Visualization","page":"Tutorial","title":"Example 3: Visualization","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The visualization is provided by the LuxorTensorPlot extension. To use it, just load the extension by using LuxorGraphPlot.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"pkg> add OMEinsumContractionOrders, LuxorGraphPlot\n\njulia> using OMEinsumContractionOrders, LuxorGraphPlot","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The extension provides the following two functions, viz_eins and viz_contraction, where the former will plot the tensor network as a graph, and the latter will generate a video or gif of the contraction process.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here is an example:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> using OMEinsumContractionOrders, LuxorGraphPlot\n\njulia> eincode = OMEinsumContractionOrders.EinCode([['a', 'b'], ['a', 'c', 'd'], ['b', 'c', 'e', 'f'], ['e'], ['d', 'f']], ['a'])\nab, acd, bcef, e, df -> a\n\njulia> viz_eins(eincode, filename = \"eins.png\")\n\njulia> nested_eins = optimize_code(eincode, uniformsize(eincode, 2), GreedyMethod())\nab, ab -> a\n├─ ab\n└─ acf, bcf -> ab\n   ├─ acd, df -> acf\n   │  ├─ acd\n   │  └─ df\n   └─ bcef, e -> bcf\n      ├─ bcef\n      └─ e\n\n\njulia> viz_contraction(nested_code)\n[ Info: Generating frames, 7 frames in total\n[ Info: Creating video at: /var/folders/3y/xl2h1bxj4ql27p01nl5hrrnc0000gn/T/jl_SiSvrH/contraction.mp4\n\"/var/folders/3y/xl2h1bxj4ql27p01nl5hrrnc0000gn/T/jl_SiSvrH/contraction.mp4\"","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The resulting image and video will be saved in the current working directory, and the image is shown below:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"<div style=\"text-align:center\">\n\t<img src=\"/assets/eins.png\" alt=\"Image\" width=\"40%\" />\n</div>","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The large white nodes represent the tensors, and the small colored nodes represent the indices, red for closed indices and green for open indices.","category":"page"},{"location":"optimizers/#Choosing-Optimizers","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Supported solvers include:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Optimizer Description\nGreedyMethod Fast, but poor contraction order\nTreeSA Reliable, local search based optimizer [Kalachev2021], but is a bit slow\nHyperND Nested dissection algorithm, similar to KaHyParBipartite. Requires importing either KaHyPar or Metis.\nKaHyParBipartite and SABipartite Graph bipartition based, suited for large tensor networks [Gray2021], requires using KaHyPar package. Alternatively, a simulated annealing bipartition method is provided in SABipartite.\nExactTreewidth (alias of Treewidth{RuleReduction{BT}}) Exact, but takes exponential time [Bouchitté2001], based on package TreeWidthSolver.\nTreewidth Tree width solver based, based on package CliqueTrees, performance is elimination algorithm dependent.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The KaHyParBipartite is implemented as an extension. If you have issues in installing KaHyPar, please check these issues: #12 and #19.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"There is a tradeoff between the time and the quality of the contraction order. The following figure shows the Pareto front of the multi-objective optimization of the time to optimize the contraction order and the time to contract the tensor network.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"(Image: )","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Among these methods, the ExactTreewidth method produces the lowest treewidth, but it does not scale up to tensor networks with more than 50 tensors. The TreeSA is the second best in terms of the treewidth. It works well in most cases, and supports slicing. The only limitation is that it is a bit slow. For application sensitive to overhead, the GreedyMethod and Treewidth method (blue region) are recommended. The Treewidth method is a zoo of methods provided by the package CliqueTrees, which is a collection of methods for finding the approximate tree decomposition of a graph. Most of them have similar performance with the GreedyMethod, and most of them are very efficient. The HyperND method has a very good overall performance in benchmarks (to be added), and it is much faster than the TreeSA method. It relies on the KaHyPar package, which is platform picky.","category":"page"},{"location":"optimizers/#Sec_GreedyMethod","page":"Choosing Optimizers","title":"GreedyMethod","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Implemented as GreedyMethod in the package. The Greedy method is one of the simplest and fastest method for optimizing the contraction order. The idea is to greedily select the pair of tensors with the smallest cost to contract at each step. The cost is defined as:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"L = textsize(textout) - α times (textsize(textin_1) + textsize(textin_2))","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"where textout is the output tensor, and textin_1 and textin_2 are the input tensors. α is a hyperparameter, which is set to 00 by default, meaning that we greedily select the pair of tensors with the smallest size of the output tensor. For alpha = 1, the size increase in each step is greedily optimized.","category":"page"},{"location":"optimizers/#Sec_TreeSA","page":"Choosing Optimizers","title":"TreeSA","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Implemented as TreeSA in the package. The local search method [Kalachev2021] is a heuristic method based on the idea of simulated annealing. The method starts from a random contraction order and then applies the following four possible transforms as shown in the following figure","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"(Image: )","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"They correspond to the different ways to contract three sub-networks:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"(A * B) * C = (A * C) * B = (C * B) * A \nA * (B * C) = B * (A * C) = C * (B * A)","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"where we slightly abuse the notation \"*\" to denote the tensor contraction, and A B C are the sub-networks to be contracted. Due to the commutative property of the tensor contraction, such transformations do not change the result of the contraction. Even through these transformations are simple, all possible contraction orders can be reached from any initial contraction order. The local search method starts from a random contraction tree. In each step, the above rules are randomly applied to transform the tree and then the cost of the new tree is evaluated, which is defined as","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"mathcalL = texttc + w_s times textsc + w_textrw times textrwc","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"where w_s and w_textrw are the weights of the space complexity and read-write complexity compared to the time complexity, respectively. The optimal choice of weights depends on the specific device and tensor contraction algorithm. One can freely tune the weights to achieve a best performance for their specific problem. Then the transformation is accepted with a probability given by the Metropolis criterion, which is","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"p_textaccept = min(1 e^-beta Delta mathcalL)","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"where beta is the inverse temperature, and Delta mathcalL is the difference of the cost of the new and old contraction trees. During the process, the temperature is gradually decreased, and the process stop when the temperature is low enough. Additionally, the TreeSA method supports the slicing technique. When the space complexity is too large, one can loop over a subset of indices, and then contract the intermediate results in the end. Such technique can reduce the space complexity, but slicing n indices will increase the time complexity by 2^n.","category":"page"},{"location":"optimizers/#Sec_HyperND","page":"Choosing Optimizers","title":"HyperND","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Implemented as HyperND in the package.","category":"page"},{"location":"optimizers/#Sec_Bipartite","page":"Choosing Optimizers","title":"KaHyParBipartite and SABipartite","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Implemented as KaHyParBipartite and SABipartite in the package. A given tensor network can be regarded as a hypergraph, where the tensors are the vertices and the shared indices are the hyperedges, and the cost of contracting a hyper edge can be encoded as its weight. The binary partition method is to partition the hypergraph into two parts, and then recursively partition each part. Cost of each partition can be evaluated by the sum of the weights of the hyperedges cut by the partition, while we prefer to make the partition as balanced as possible (balance means size of the subgraph should be similar). Thus, the problem is reduced to a balanced min cut problem on a hypergraph. In the past few decades, the graph community has developed many algorithms for the balanced min cut problem and provided the corresponding software packages, such as KaHyPar[Schlag2021].","category":"page"},{"location":"optimizers/#Sec_ExactTreewidth","page":"Choosing Optimizers","title":"ExactTreewidth","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Implemented as ExactTreewidth in the package. This method is a technical note for the Google Summer of Code 2024 project \"Tensor network contraction order optimization and visualization\" released by The Julia Language, where I developed a package TreeWidthSolver.jl for calculating the tree decomposition with minimal treewidth of a given simple graph and made it a backend of OMEinsumContracionOrders.jl.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"In the previous section, we introduce the concept of tensor network and its contraction order, so that now you should understand why the contraction order so important.  Then the next question is how to find the optimal contraction order.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"In our work, we propose to use the tree decomposition of the line graph of the hypergraph representation of the tensor network to find the optimal contraction order, according to the following well known theoremMarkov :","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Theorem 1. Let C be a quantum circuit with T gates and whose underlying circuit graph is G_c. Then C can be simulated deterministically in time T^O(1) e^O(tw(G_C)), where tw(G_C) is the treewidth of G_C.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Using the language of tensor network, we can rewrite the above theorem as follows: the bottleneck of time complexity of the contraction of a tensor network is O(e^O(tw(L(G)))), where L(G) is the line graph of the hypergraph representation of the tensor network.  Therefore, if we can find the tree decomposition of the tensor network with minimal treewidth, we can find the optimal contraction order of the tensor network. We developed a package TreeWidthSolver.jl for finding the optimal tree decomposition of a given simple graph, which can be used as a backend of OMEinsumContractionOrders.jl. For more details about the tree decomposition and its relation to the contraction order, please refer to the appendix.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Here is an example of usage:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"julia> using OMEinsum, OMEinsumContractionOrders\n\n# define the contraction using Einstein summation\njulia> code = ein\"ijl, ikm, jkn, l, m, n -> \"\nijl, ikm, jkn, l, m, n -> \n\nulia> optimizer = ExactTreewidth()\nExactTreewidth{GreedyMethod{Float64, Float64}}(GreedyMethod{Float64, Float64}(0.0, 0.0, 1))\n\n# set the size of the indices\njulia> size_dict = uniformsize(code, 2)\nDict{Char, Int64} with 6 entries:\n  'n' => 2\n  'j' => 2\n  'i' => 2\n  'l' => 2\n  'k' => 2\n  'm' => 2\n\njulia> optcode = optimize_code(code, size_dict, optimizer)\nn, n -> \n├─ jk, jkn -> n\n│  ├─ ij, ik -> jk\n│  │  ├─ ijl, l -> ij\n│  │  │  ├─ ijl\n│  │  │  └─ l\n│  │  └─ ikm, m -> ik\n│  │     ├─ ikm\n│  │     └─ m\n│  └─ jkn\n└─ n\n\n# check the complexity\njulia> contraction_complexity(optcode, size_dict)\nTime complexity: 2^5.087462841250339\nSpace complexity: 2^2.0\nRead-write complexity: 2^5.882643049361841\n\n# check the results\njulia> A = rand(2, 2, 2); B = rand(2, 2, 2); C = rand(2, 2, 2); D = rand(2); E = rand(2); F = rand(2);\n\njulia> code(A, B, C, D, E, F) ≈ optcode(A, B, C, D, E, F)\ntrue","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"This optimizer will be used as an extension of TensorOperations.jl in the future, see this PR. We compared the performance of this method against the default optimizer of TensorOperations.jl based on exhaustive searching, the results is shown below.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"(Image: )","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The results shown that the tree width based solver is faster for some graph similar to trees. For more details, please see the benchmark repo: https://github.com/ArrogantGao/TreeWidthSolver_benchmark.","category":"page"},{"location":"optimizers/#The-Bouchitté–Todinca-Algorithm-for-Exact-Tree-Width","page":"Choosing Optimizers","title":"The Bouchitté–Todinca Algorithm for Exact Tree Width","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Various algorithms have been developed to calculate the treewidth of a graph in the past few decades, both exactly and approximately. In this section, I will introduce one of the most basic exact algorithms: the Bouchitté–Todinca (BT) algorithm Bouchitte BouchitteListing Tuukka, which makes use of the theory of minimal triangulations, characterizing the minimal triangulations of a graph via objects called minimal separators and potential maximal cliques of the graph.","category":"page"},{"location":"optimizers/#Triangulation-and-Minimal-Triangulation","page":"Choosing Optimizers","title":"Triangulation and Minimal Triangulation","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"First of all, let's introduce the concept of triangulation and minimal triangulation.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Triangulations of graphs are a central graph-theoretic concept in the computation of tree decompositions.  Triangulations are defined via chordality of graphs.  A graph G is chordal if every cycle in G with at least 4 vertices contains a chord, which is an edge that is not part of the cycle but connects two vertices of the cycle Correspondingly, a non-chordal graph has at least one chordless cycle, i.e., a cycle with at least 4 vertices that does not have a chord.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Definition 2 (triangulation). A graph H is a triangulation of a graph G if H is chordal, V (G) = V (H), and E(G) subseteq E(H).","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Here is an example of a triangulation of a graph:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"(Image: )","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"where the left graph is the original graph and the right one is a triangulation of the graph.","category":"page"},{"location":"optimizers/#Minimal-Separator-and-Potential-Maximal-Cliques","page":"Choosing Optimizers","title":"Minimal Separator and Potential Maximal Cliques","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Then we can introduce the concept of minimal separator and potential maximal cliques.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"A set of vertices S subseteq V (G) is an ab-separator of a graph G if the vertices a and b are in different connected components of G setminus S.  In other words, all paths between a and b go through S. The set S is a minimal ab-separator of G if no subset of S is also an ab-separator. The minimal separator of a graph is defined as follows:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Defination 3 (minimal separator): Let G be a graph. A set of vertices S subseteq V (G) is a minimal separator of G if it is a minimal ab-separator for some pair a b in V (G).","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"For example, in the graph shown above, the set B C is a minimal separator of the graph, which separates the graph into two disconnected parts: A and D E F G H. It is also easy to see that the set BC is exactly the intersection of the two neighboring bags A B C and B C E in the tree decomposition. Actually, all intersection of neighboring bags in a tree decomposition is a separator of the graph.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"(Image: )","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Defination 4 (potential maximal clique): A set of vertices Omega subseteq V (G) is a potential maximal clique of a graph G if there is a minimal triangulation H of G such that Omega is a maximal clique of H. A set of vertices is a maximal clique if it is a clique and no strict superset of it is a clique.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"For example, in the graph shown above, the sets BCE, BGE, BCG and C E G are all potential maximal cliques of the graph, corresponding to different triangulations of the graph:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"(Image: )","category":"page"},{"location":"optimizers/#The-Bouchitté–Todinca-Dynamic-Programming-Algorithm","page":"Choosing Optimizers","title":"The Bouchitté–Todinca Dynamic Programming Algorithm","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The reason that we are interested in minimal separators and potential maximal cliques the following properties of the optimal tree decomposition:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"All tree bags of a tree decomposition with minimal treewidth are potential maximal cliques of the graph.\nThe intersection of any two neighboring bags in a tree decomposition is a minimal separator of the graph.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Based on these properties, the Bouchitté–Todinca algorithm first calculates all minimal separators and potential maximal cliques of the graph, and then uses dynamic programming to find a set of potential maximal cliques that minimizes the width of the tree decomposition.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"In this section, I will introduce the Bouchitté–Todinca algorithm in detail, which can be separated into the following steps:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"List all minimal separators, Delta;\nList all potential maximal cliques, Pi;\nCalculate the treewidth of the graph.","category":"page"},{"location":"optimizers/#Step-1:-List-all-minimal-separators","page":"Choosing Optimizers","title":"Step 1: List all minimal separators","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"To recognize minimal separators of a graph, we mainly use the following property.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"For a set of vertices S, consider the connected components of G setminus S, represented as mathcalC(G setminus S), which are called the components of S. For C in mathcalC(G setminus S), if N(C) = S, then C is called a full-component of S.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Theorem 1 (minimal separator): The set S is a minimal separator if and only if it has two or more than two full-components.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"For example, B C is a minimal separator of the graph shown above, with two full-components A and D E F G H. While B C F is not, since it has only one full-component D E G H.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"It is shown that the following proposition holds:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Proposition 1: Let S be a minimal separator of a graph G and a a vertex of G, then neighbors of the connected components of G setminus (S cup a), i.e. $ \\mathcal{R}(S) = {N(C) | C \\in \\mathcal{C}(G \\setminus (S \\cup {a}))} $ are all minimal separators of the graph.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Then we start from all vertices v of the graph and theirs neighbor N(v), and repeatedly apply the proposition above to list all minimal separators of the graph.","category":"page"},{"location":"optimizers/#Step-2:-List-all-potential-maximal-cliques","page":"Choosing Optimizers","title":"Step 2: List all potential maximal cliques","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"In the second step we list all potential maximal cliques of the graph using Delta calculated in the first step. To check a set of vertices Omega is a potential maximal clique, we can use the following property:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Theorem 2 (potential maximal clique): Let Omega be a set of vertices of a graph G. The set Omega is a potential maximal clique if and only if the following conditions hold:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"for any pair of vertices a b in Omega, either a and b are connected by an edge in the graph, or there is a minimal separator S such that a in S and b in S;\nno component of Omega is full;","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"where the first one is called the cliquish condition, and the second one is called the non-full condition.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Then, one can use the following rules to list all potential maximal cliques: ","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Proposition 2: Let Omega be a potential maximal clique and a a vertex of G, and a is a vertex of G. If V(G) geq 2, one of the following conditions holds:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Omega setminus a in Pi(G setminus a)\n;\nOmega setminus a in Delta(G)\n;\nOmega = S cup T setminus a\n, where S and T are minimal separators of G such that a notin S, S notin Delta(G setminus a) and a in T.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The proposition indicates that with Pi(G setminus a), Delta(G setminus a) and Delta(G), one can construct Pi(G) by adding one vertex a to the graph.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Then one can iteratively construct the graph by adding one vertex each time, here we represent the i-th graph as G_i = G(v_1 v_2 cdots v_i), and then calculate Pi(G_i) using Pi(G_i-1), Delta(G_i-1) and Delta(G_i) according to the proposition above, until i = N and all potential maximal cliques are found.","category":"page"},{"location":"optimizers/#Step-3:-Calculate-the-treewidth-of-the-graph","page":"Choosing Optimizers","title":"Step 3: Calculate the treewidth of the graph","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Finally, we can calculate the treewidth of the graph using Delta and Pi calculated in the first two steps.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The BT algorithm is based on the following two ideas: ","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"all tree bags of the tree decomposition with minimal treewidth are potential maximal cliques of the graph;\ntree width of a graph is larger than that of its subgraphs.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Then for a given potential maximal clique Omega, the minimal treewidth among all tree decompositions with Omega is given by tw(Omega) = max(Omega - 1 tw(G(C_i cup S_i)))C_i in mathcalC(G setminus Omega)S_i = C_i cap Omega and the treewidth of the graph is the minimum of tw(Omega) among all potential maximal cliques Omega.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"For example, in the figure below, we assume that the potential maximal clique Omega = B C E with width 2 is in the decomposition, and then we need to compare that against the width of the subgraphs G(A B C), G(C D E) and G(B E F G H). Since A B C and C D E are already potential maximal cliques, theirs width is 2; and for G(B E F G H), we can apply a similar procedure to calculate the width of the subgraphs, which is also 2. Thus tw(B C E) = 2. By comparing width of all possible choices of Omega, we can find the treewidth of the graph.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The BT algorithm first calculates all possible G(C cup S) for all Omega and sort the triplets (Omega C S) according to size of C cup S. Then with the help of dynamic programming, the algorithm calculate width of subgraph G(C cup S) from the smallest to the largest. In each step, treewidth of all possible subgraphs of the current graph G(C cup S) is already calculated, so that the treewidth of G(C cup S) can be directly obtain by comparing the width of the subgraphs and Omega - 1.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Furthermore, if the choice of Omega of each step is stored, the tree decomposition can be easily obtained by connecting these potential maximal cliques.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Using the BT algorithm, one can calculate the treewidth of a graph exactly, and the algorithm has a time complexity of O(Pinm), which are dependent on the graph structure.","category":"page"},{"location":"optimizers/#Sec_Treewidth","page":"Choosing Optimizers","title":"Treewidth","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Implemented as Treewidth in the package.","category":"page"},{"location":"optimizers/#Exhaustive-Search-(planned)","page":"Choosing Optimizers","title":"Exhaustive Search (planned)","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The exhaustive search [Robert2014] is a method to get the exact optimal contraction complexity. There are three different ways to implement the exhaustive search:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Depth-first constructive approach: in each step, choose a pair of tensors to contract a new tensor until all tensors are contracted, and then iterate over all possible contraction sequences without duplication. Note the cheapest contraction sequence thus found.\nBreadth-first constructive approach: the breadth-first method construct the set of intermediate tensors by contracting c tensors (c in 1 n - 1, where n is the number of tensors) in each step, and record the optimal cost for constructing each intermediate tensor. Then in the last step, the optimal cost for contracting all n tensors is obtained.\nDynamic programming: in each step, consider all bipartition that split the tensor network into two parts, if the optimal cost for each part is not recorded, further split them until the cost has been already obtained or only one tensor is left. Then combine the two parts and record the optimal cost of contracting the sub-networks. In this end the optimal cost for the whole network is obtained.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"In more recent work [Robert2014], by reordering the search process in favor of cheapest-first and excluding large numbers of outer product contractions which are shown to be unnecessary, the efficiency of the exhaustive search has been greatly improved. The method has been implemented in TensorOperations.jl.","category":"page"},{"location":"optimizers/#Performance-Benchmark","page":"Choosing Optimizers","title":"Performance Benchmark","text":"","category":"section"},{"location":"optimizers/#Compare-ExactTreewidth-and-exhaustive-search","page":"Choosing Optimizers","title":"Compare ExactTreewidth and exhaustive search","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"We benchmarked the package on a set of random graphs with different sizes, including the 3-regular graph, line graph and random tree graph, and the results are shown below:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"(Image: )","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"where n is the number of vertices of the graph, T is the time used to calculate the treewidth of the graph. Results for the 3-regular graph and random tree graph are averaged over 10 samples. The results show that for different types of graphs, the time used to calculate the treewidth can be quite different, and the time complexity of the algorithm is dependent on the graph structure. For simple line/tree graphs, the BT algorithm can reach a polynomial time complexity and calculate the treewidth of the graph in a short time, while for more complex graphs, the time used can be much longer. It has been proved that the upper bound of this algorithm is about O(17^n).","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The following figure shows a comparison with the exhaustive search in TensorOperations.jl:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"(Image: )","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The results shown that the tree width based solver is faster for some graph similar to trees. For more details, please see the benchmark repo: https://github.com/ArrogantGao/TreeWidthSolver_benchmark.","category":"page"},{"location":"optimizers/#References","page":"Choosing Optimizers","title":"References","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"[Bouchitté2001]: Bouchitté, V., Todinca, I., 2001. Treewidth and Minimum Fill-in: Grouping the Minimal Separators. SIAM J. Comput. 31, 212–232. https://doi.org/10.1137/S0097539799359683","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"[Gray2021]: Gray, Johnnie, and Stefanos Kourtis. \"Hyper-optimized tensor network contraction.\" Quantum 5 (2021): 410.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"[Kalachev2021]: Kalachev, Gleb, Pavel Panteleev, and Man-Hong Yung. \"Recursive multi-tensor contraction for XEB verification of quantum circuits.\" arXiv preprint arXiv:2108.05665 (2021).","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"[Robert2014]: Pfeifer, R.N.C., Haegeman, J., Verstraete, F., 2014. Faster identification of optimal contraction sequences for tensor networks. Phys. Rev. E 90, 033315. https://doi.org/10.1103/PhysRevE.90.033315","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"[Schlag2021]: Schlag, S., Heuer, T., Gottesbüren, L., Akhremtsev, Y., Schulz, C., Sanders, P., 2021. High-Quality Hypergraph Partitioning. https://doi.org/10.48550/arXiv.2106.08696","category":"page"}]
}
