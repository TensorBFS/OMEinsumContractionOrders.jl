var documenterSearchIndex = {"docs":
[{"location":"background/#Tensor-Network-Contraction-Order-Optimization","page":"Background Knowledge","title":"Tensor Network Contraction Order Optimization","text":"","category":"section"},{"location":"background/#Tensor-network","page":"Background Knowledge","title":"Tensor network","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Tensor network is a powerful tool for modeling and simulating quantum many-body systems, probabilistic inference, combinatorial optimization, etc. It is a diagrammatic representation of tensor contractions. In this representation, a tensor is represented as a node, and an index is represented as a hyperedge (a hyperedge can connect to any number of nodes). For example, vectors, matrices and higher order tensors can be represented as:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: )","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"To understand the basic concepts of tensor network, we recommend the following references for readers with different background:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"For readers with physics background: https://tensornetwork.org/diagrams/\nFor readers want to get a formal definition: Chapter 2 of https://epubs.siam.org/doi/abs/10.1137/22M1501787","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"The defining operation of a tensor network is contraction, which is also named as sum-product operation. It can be viewed as the generalization of matrix multiplication to tensors. We illustrate tensor network contraction with the following example.","category":"page"},{"location":"background/#Example:-Trace-permutation-rule","page":"Background Knowledge","title":"Example: Trace permutation rule","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Let A B and C be three square matrices with the same size. The trace operation texttr(A B C) is defined as the sum of the products of the elements of the matrices:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"texttr(A B C) = sum_ijk A_ij B_ik C_jk","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"It can be represented as a tensor network diagram as follows:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: )","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"So, each tensor is represented as a node, and each index is represented as an edge. Tensors sharing the same index are connected by the edge. The contraction of the tensor network is the summation of the products of the elements of the tensors over all possible values of the indices.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"The contraction can happen in different orders. The famous trace permutation rule states that the trace is invariant under cyclic permutations of the matrices:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"texttr(A B C) = texttr(C A B) = texttr(B C A)","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Hence, we observe that tensor network contraction is associative and commutative, i.e., the order of evaluation does not change the result. In the tensor network diagram, the order of evaluation is neglected, so the trace permutation rule is trivially satisfied.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"By drawing diagrams, we prove the trace permutation rule. Isn't it cool?","category":"page"},{"location":"background/#Einsum-notation","page":"Background Knowledge","title":"Einsum notation","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"For simplicity, in the following sections, we will use the einsum notation in OMEinsum.jl to represent the tensor network contraction. It is almost the same as the numpy einsum notation. For example, the contraction of the tensor network in the previous trace permutation example can be represented as:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"julia> using OMEinsum\n\njulia> trace_perm = ein\"ij, jk, ki -> \"\nij, jk, ki ->","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"The string ij, jk, ki -> is the einsum notation, where labels of the input tensors and the output tensor are separated by ->, and labels of different input tensors are separated by ,. Here, ij, jk, ki are the labels of the input tensors of rank 2 (matrices), and the label of the output tensor is empty, representing a scalar.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Einsum notation is a powerful tool to represent linear operations, e.g. batched matrix multiplication is represented as ijb,jkb->ikb, taking diagonal part of a matrix is represented as ii->i. Sometimes, a single notation may have thousands or more tensors. Then the contraction order becomes relevant to computational cost of evaluating it.","category":"page"},{"location":"background/#Contraction-order-and-complexity","page":"Background Knowledge","title":"Contraction order and complexity","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Contraction order is a key factor for the performance of tensor network contraction. It is represented by a binary tree, where the leaves are the tensors to be contracted and the internal nodes are the intermediate tensors. The quality of the contraction order is quantified by the contraction complexity, which consists of the following metrics","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"time complexity: the number of floating point operations required to calculate the result;\nspace complexity: the largest size of the intermediate tensors. For larger tensor networks, the contraction order is important, since it can greatly reduce the time complexity of the calculation.\nread-write complexity: the number of times the intermediate tensors are read and written.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"The contraction order optimization aims to find the optimal contraction order with the lowest cost, which is usually defined as some linear combination of the above complexities.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Finding the optimal contraction order is NP-complete, but fortunately, a close-to-optimal contraction order is usually good enough, which could be found in a reasonable time with a heuristic optimizer. In the past decade, methods have been developed to optimize the contraction orders, including both exact ones and heuristic ones. Among these methods, multiple heuristic methods can handle networks with more than 10^4 tensors efficiently [Gray2021], [Roa2024].","category":"page"},{"location":"background/#Example:-Optimizing-the-contraction-order-with-OMEinsum.jl","page":"Background Knowledge","title":"Example: Optimizing the contraction order with OMEinsum.jl","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Considering the following simple tensor network:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"julia> einsum = ein\"ij, ik, jl, lk -> \"\nij, ik, jl, lk ->","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: )","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Here we simply assume that all indices are of the same dimension D. Then the naive way to calculate the result is to loop over all the indices, which requires O(D^4) operations and no intermediate tensors are produced.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Alternatively, we can contract step by step. We first contraction tensors A B mapsto A B, and C D mapsto C D, which produces two rank-2 intermediate tensors, and then contract A B with C D to get the scalar s.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: )","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"It is equivalent to the following einsum notation:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"julia> nested_ein = ein\"(ij, ik), (jl, lk) -> \"\njk, jk ->\n├─ ij, ik -> jk\n│  ├─ ij\n│  └─ ik\n└─ jl, lk -> jk\n   ├─ jl\n   └─ lk","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"In this way, the total number of operations is O(2 D^3 + D^2), which is smaller than the naive calculation, while the trade-off is that we need to store the intermediate tensors AB and CD with size of O(D^2), as shown below:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"# here we take D = 16\njulia> size_dict = uniformsize(einsum, 2^4)\n\njulia> contraction_complexity(einsum, size_dict)\nTime complexity: 2^16.0\nSpace complexity: 2^0.0\nRead-write complexity: 2^10.001408194392809\n\njulia> contraction_complexity(nested_ein, size_dict)\nTime complexity: 2^13.044394119358454\nSpace complexity: 2^8.0\nRead-write complexity: 2^11.000704269011246","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"We say such a contraction is with time complexity of O(D^3) and space complexity of O(D^4).","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"In actual calculation, we prefer binary contractions, i.e., contracting two tensors at a time, by converting these two tensors as matrices, so that we can make use of BLAS libraries to speed up the calculation. In this way, a given contraction order can be represented as a binary tree. The contraction tree can be represented as a rooted tree, where the leaves are the tensors to be contracted and the internal nodes are the intermediate tensors. The contraction tree corresponding to the above example is shown below:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: )","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Generally speaking, our target is to find a binary contraction order, with minimal time complexity or space complexity, which is called the optimal contraction order.","category":"page"},{"location":"background/#Tree-decomposition-and-tree-width","page":"Background Knowledge","title":"Tree decomposition and tree width","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Finding the optimal contraction order is related to another NP-complete problem: finding the tree decomposition of a graph with minimal treewidth [Markov2008]. Tree width is a graph characteristic that measures how similar a graph is to a tree, the lower the tree width, the more similar the graph is to a tree. The way to relate a graph to a tree is to find a tree decomposition of the graph, which is a tree whose nodes are subsets of the vertices of the graph, and the following conditions are satisfied:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Each vertex of the graph is in at least one node of the tree.\nFor each edge of the graph, there is a node of the tree containing both vertices of the edge.\nBags containing the same vertex have to be connected in the tree.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"All the nodes of the tree are called tree bags, and intersection of two bags is called a separator. The width of a tree decomposition is the size of the largest bag minus one. Clearly, one graph can have multiple tree decomposition with different corresponding widths. The tree width of a graph is the minimal width of all such decompositions, and a particular decomposition (not necessarily unique) that realises this minimal width is called an optimal tree decomposition. Its width is the tree width of the graph, denoted as texttw(G).","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Let G be the hypergraph topology of a tensor network, where a tensor is mapped to a vertex and an index is mapped to an edge. Two tensors are connected if they share a common index. Instead, if we treat each index as a vertex and each tensor as an edge, we can get its line graph L(G). Ref.[Markov2008] shows that the bottleneck time complexity of the contraction of a tensor network is O(2^texttw(L(G))), where texttw(L(G)) is the treewidth of L(G). Therefore, if we can find the tree decomposition of the tensor network with minimal treewidth, we can find the optimal contraction order of the tensor network.","category":"page"},{"location":"background/#Example:-Line-graph-and-tree-decomposition","page":"Background Knowledge","title":"Example: Line graph and tree decomposition","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Consider the einsum notation:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"julia> ein\"ABC,BFG,EGH,CDE->\"","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Its graph G, its line graph L(G) and its tree decomposition are shown in the following figure:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: Fig.1)","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"The tree decomposition in figure (c) is related to the contraction order of the tensor network in the following way:","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"By defintion of tree decomposition, each edge of the line graph is contained in at least one tree bag. Hence, every tensor (represented as a hyperedge in L(G)) can fit into a tree bag.\nWe contract the tensor networks from the leaves to the root. Tensor network contraction requires the indices used in the future steps must be kept in the output tensor. This is automatically satisfied by the third requirement of tree decomposition: If two bags containing two tensors sharing the same index, then this index must appear in all bags between them, such that they can be connected.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"As a remark, the tree decomposition considered here handles with weighted vertices. Since we are considering a tensor network, dimension of the indices have to be considered.  Therefore, for each vertex of the line graph L(G), we define its weight as log_2(d), where d is the dimension of the index. In this way, size of a tensor can be represented as the sum of weights of the vertices in L(G).","category":"page"},{"location":"background/#Reduce-space-complexity-by-slicing","page":"Background Knowledge","title":"Reduce space complexity by slicing","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"Slicing is a technique to reduce the space complexity of the tensor network by looping over a subset of indices. This effectively reduces the size of the tensor network inside the loop, and the space complexity can potentially be reduced. For example, in the following figure, we slice the tensor network over the index i. The label i is removed from the tensor network, at the cost of contraction multiple tensor networks.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"(Image: )","category":"page"},{"location":"background/#References","page":"Background Knowledge","title":"References","text":"","category":"section"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"[Gray2021]: Gray, Johnnie, and Stefanos Kourtis. \"Hyper-optimized tensor network contraction.\" Quantum 5 (2021): 410.","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"[Markov2008]: Markov, I.L., Shi, Y., 2008. Simulating Quantum Computation by Contracting Tensor Networks. SIAM J. Comput. 38, 963–981. https://doi.org/10.1137/050644756","category":"page"},{"location":"background/","page":"Background Knowledge","title":"Background Knowledge","text":"[Roa2024]: Roa-Villescas, M., Gao, X., Stuijk, S., Corporaal, H., Liu, J.-G., 2024. Probabilistic Inference in the Era of Tensor Networks and Differential Programming. Phys. Rev. Research 6, 033261. https://doi.org/10.1103/PhysRevResearch.6.033261","category":"page"},{"location":"ref/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"ref/#Data-structures-and-interfaces","page":"Reference","title":"Data structures and interfaces","text":"","category":"section"},{"location":"ref/#OMEinsumContractionOrders.AbstractEinsum","page":"Reference","title":"OMEinsumContractionOrders.AbstractEinsum","text":"AbstractEinsum\n\nAbstract type for einsum notations.\n\nRequired Interfaces\n\ngetixsv: a vector of vectors, each vector represents the labels associated with a input tensor.\ngetiyv: a vector of labels associated with the output tensor.\nuniquelabels: a vector of labels that are unique in the einsum notation.\n\nDerived interfaces\n\nlabeltype: the data type to represent the labels in the einsum notation.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.CodeOptimizer","page":"Reference","title":"OMEinsumContractionOrders.CodeOptimizer","text":"CodeOptimizer\n\nAbstract type for code optimizers.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.CodeSlicer","page":"Reference","title":"OMEinsumContractionOrders.CodeSlicer","text":"CodeSlicer\n\nAbstract type for code slicers.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.EinCode","page":"Reference","title":"OMEinsumContractionOrders.EinCode","text":"EinCode{LT} <: AbstractEinsum\nEinCode(ixs::Vector{Vector{LT}}, iy::Vector{LT})\n\nEinsum code with input indices ixs and output index iy.\n\nExamples\n\nThe einsum notation for matrix multiplication is:\n\njulia> code = OMEinsumContractionOrders.EinCode([[1,2], [2, 3]], [1, 3])\n1∘2, 2∘3 -> 1∘3\n\njulia> OMEinsumContractionOrders.getixsv(code)\n2-element Vector{Vector{Int64}}:\n [1, 2]\n [2, 3]\n\njulia> OMEinsumContractionOrders.getiyv(code)\n2-element Vector{Int64}:\n 1\n 3\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.NestedEinsum","page":"Reference","title":"OMEinsumContractionOrders.NestedEinsum","text":"NestedEinsum{LT} <: AbstractEinsum\nNestedEinsum(args::Vector{NestedEinsum}, eins::EinCode)\n\nThe einsum notation with a contraction order specified as a tree data structure. It is automatically generated by the contraction code optimizer with the optimize_code function.\n\nFields\n\nargs: the children of the current node\ntensorindex: the index of the input tensor, required only for leaf nodes. For non-leaf nodes, it is -1.\neins: the einsum notation for the operation at the current node.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.ScoreFunction","page":"Reference","title":"OMEinsumContractionOrders.ScoreFunction","text":"ScoreFunction\n\nA function to compute the score of a contraction code:\n\nscore = tc_weight * 2^tc + rw_weight * 2^rw + sc_weight * max(0, 2^sc - 2^sc_target)\n\nFields\n\ntc_weight: the weight of the time complexity, default is 1.0.\nsc_weight: the weight of the space complexity (the size of the largest tensor), default is 1.0.\nrw_weight: the weight of the read-write complexity, default is 0.0.\nsc_target: the target space complexity, below which the sc_weight will be set to 0 automatically, default is 0.0.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.SlicedEinsum","page":"Reference","title":"OMEinsumContractionOrders.SlicedEinsum","text":"SlicedEinsum{LT,ET<:Union{EinCode{LT},NestedEinsum{LT}}} <: AbstractEinsum\nSlicedEinsum(slicing::Vector{LT}, eins::ET)\n\nThe einsum notation with sliced indices. The sliced indices are the indices enumerated manually at the top level. By slicing the indices, the space complexity of the einsum notation can be reduced.\n\nFields\n\nslicing: the sliced indices.\neins: the einsum notation of the current node, which is a NestedEinsum object.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.getixsv","page":"Reference","title":"OMEinsumContractionOrders.getixsv","text":"getixsv(code::AbstractEinsum) -> Vector{Vector{LT}}\n\nReturns the input indices of the einsum notation. Each vector represents the labels associated with a input tensor.\n\n\n\n\n\n","category":"function"},{"location":"ref/#OMEinsumContractionOrders.getiyv","page":"Reference","title":"OMEinsumContractionOrders.getiyv","text":"getiyv(code::AbstractEinsum) -> Vector{LT}\n\nReturns the output index of the einsum notation.\n\n\n\n\n\n","category":"function"},{"location":"ref/#OMEinsumContractionOrders.labeltype-Tuple{OMEinsumContractionOrders.AbstractEinsum}","page":"Reference","title":"OMEinsumContractionOrders.labeltype","text":"labeltype(code::AbstractEinsum) -> Type\n\nReturns the data type to represent the labels in the einsum notation.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.uniquelabels-Tuple{OMEinsumContractionOrders.AbstractEinsum}","page":"Reference","title":"OMEinsumContractionOrders.uniquelabels","text":"uniquelabels(code::AbstractEinsum) -> Vector{LT}\n\nReturns the unique labels in the einsum notation. The labels are the indices of the tensors.\n\n\n\n\n\n","category":"method"},{"location":"ref/#Time-and-space-complexity","page":"Reference","title":"Time and space complexity","text":"","category":"section"},{"location":"ref/#OMEinsumContractionOrders.contraction_complexity-Tuple{OMEinsumContractionOrders.AbstractEinsum, Any}","page":"Reference","title":"OMEinsumContractionOrders.contraction_complexity","text":"contraction_complexity(eincode, size_dict) -> ContractionComplexity\n\nReturns the time, space and read-write complexity of the einsum contraction. The returned ContractionComplexity object contains 3 fields:\n\ntc: time complexity defined as log2(number of element-wise multiplications).\nsc: space complexity defined as log2(size of the maximum intermediate tensor).\nrwc: read-write complexity defined as log2(the number of read-write operations).\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.flop-Union{Tuple{VT}, Tuple{LT}, Tuple{OMEinsumContractionOrders.EinCode, Dict{LT, VT}}} where {LT, VT}","page":"Reference","title":"OMEinsumContractionOrders.flop","text":"flop(eincode, size_dict) -> Int\n\nReturns the number of iterations, which is different with the true floating point operations (FLOP) by a factor of 2.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.label_elimination_order-Tuple{OMEinsumContractionOrders.NestedEinsum}","page":"Reference","title":"OMEinsumContractionOrders.label_elimination_order","text":"label_elimination_order(code) -> Vector\n\nReturns a vector of labels sorted by the order they are eliminated in the contraction tree. The contraction tree is specified by code, which e.g. can be a NestedEinsum instance.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.peak_memory-Tuple{OMEinsumContractionOrders.NestedEinsum, Dict}","page":"Reference","title":"OMEinsumContractionOrders.peak_memory","text":"peak_memory(code, size_dict::Dict) -> Int\n\nEstimate peak memory in number of elements.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.uniformsize-Tuple{OMEinsumContractionOrders.AbstractEinsum, Any}","page":"Reference","title":"OMEinsumContractionOrders.uniformsize","text":"uniformsize(code::AbstractEinsum, size::Int) -> Dict\n\nReturns a dictionary that maps each label to the given size.\n\n\n\n\n\n","category":"method"},{"location":"ref/#Contraction-order-optimizers","page":"Reference","title":"Contraction order optimizers","text":"","category":"section"},{"location":"ref/#OMEinsumContractionOrders.optimize_code-Tuple{Union{OMEinsumContractionOrders.EinCode, OMEinsumContractionOrders.NestedEinsum}, Dict, CodeOptimizer}","page":"Reference","title":"OMEinsumContractionOrders.optimize_code","text":"optimize_code(eincode, size_dict, optimizer = GreedyMethod(); slicer=nothing, simplifier=nothing, permute=true) -> optimized_eincode\n\nOptimize the einsum contraction code and reduce the time/space complexity of tensor network contraction. Returns a NestedEinsum instance. Input arguments are\n\nArguments\n\neincode is an einsum contraction code instance, one of DynamicEinCode, StaticEinCode or NestedEinsum.\nsize is a dictionary of \"edge label=>edge size\" that contains the size information, one can use uniformsize(eincode, 2) to create a uniform size.\noptimizer is a CodeOptimizer instance, should be one of GreedyMethod, Treewidth, KaHyParBipartite, SABipartite or TreeSA. Check their docstrings for details.\n\nKeyword Arguments\n\nslicer is for slicing the contraction code to reduce the space complexity, default is nothing. Currently only TreeSASlicer is supported.\nsimplifier is one of MergeVectors or MergeGreedy. Default is nothing.\npermute is a boolean flag to indicate whether to optimize the permutation of the contraction order.\n\nExamples\n\njulia> using OMEinsum\n\njulia> code = ein\"ij, jk, kl, il->\"\nij, jk, kl, il -> \n\njulia> optimize_code(code, uniformsize(code, 2), TreeSA());\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.slice_code-Tuple{OMEinsumContractionOrders.NestedEinsum, Any, TreeSASlicer}","page":"Reference","title":"OMEinsumContractionOrders.slice_code","text":"slice_code(code, size_dict, slicer) -> sliced_code\n\nSlice the einsum contraction code to reduce the space complexity, returns a SlicedEinsum instance.\n\nArguments\n\ncode is a NestedEinsum instance.\nsize_dict is a dictionary of \"edge label=>edge size\" that contains the size information, one can use uniformsize(eincode, 2) to create a uniform size.\nslicer is a CodeSlicer instance, currently only TreeSASlicer is supported.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.GreedyMethod","page":"Reference","title":"OMEinsumContractionOrders.GreedyMethod","text":"GreedyMethod{MT}\nGreedyMethod(; α = 0.0, temperature = 0.0)\n\nIt may not be optimal, but it is fast.\n\nFields\n\nα is the parameter for the loss function, for pairwise interaction, L = size(out) - α * (size(in1) + size(in2))\ntemperature is the parameter for sampling, if it is zero, the minimum loss is selected; for non-zero, the loss is selected by the Boltzmann distribution, given by p ~ exp(-loss/temperature).\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.optimize_greedy-Union{Tuple{T2}, Tuple{L}, Tuple{OMEinsumContractionOrders.EinCode{L}, Dict{L, T2}}} where {L, T2}","page":"Reference","title":"OMEinsumContractionOrders.optimize_greedy","text":"optimize_greedy(eincode, size_dict; α, temperature)\n\nGreedy optimizing the contraction order and return a NestedEinsum object. Check the docstring of tree_greedy for detailed explaination of other input arguments.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.tree_greedy-Union{Tuple{ET}, Tuple{TT}, Tuple{TA}, Tuple{OMEinsumContractionOrders.IncidenceList{Int64, ET}, Any}} where {TA, TT, ET}","page":"Reference","title":"OMEinsumContractionOrders.tree_greedy","text":"tree_greedy(incidence_list, log2_sizes; α = 0.0, temperature = 0.0)\n\nCompute greedy order, and the time and space complexities, the rows of the incidence_list are vertices and columns are edges. log2_sizes are defined on edges. α is the parameter for the loss function, for pairwise interaction, L = size(out) - α * (size(in1) + size(in2)) temperature is the parameter for sampling, if it is zero, the minimum loss is selected; for non-zero, the loss is selected by the Boltzmann distribution, given by p ~ exp(-loss/temperature).\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.TreeSA","page":"Reference","title":"OMEinsumContractionOrders.TreeSA","text":"TreeSA{IT} <: CodeOptimizer\nTreeSA(; βs=collect(0.01:0.05:15), ntrials=10, niters=50, initializer=:greedy, score=ScoreFunction())\n\nOptimize the einsum contraction pattern using the simulated annealing on tensor expression tree.\n\nFields\n\nntrials, βs and niters are annealing parameters, doing ntrials indepedent annealings, each has inverse tempteratures specified by βs, in each temperature, do niters updates of the tree.\ninitializer specifies how to determine the initial configuration, it can be :greedy, :random or :specified. If the initializer is :specified, the input code should be a NestedEinsum object.\nscore specifies the score function to evaluate the quality of the contraction tree, it is a function of time complexity, space complexity and read-write complexity.\n\nReferences\n\nRecursive Multi-Tensor Contraction for XEB Verification of Quantum Circuits\n\nBreaking changes:\n\nnslices is removed, since the slicing part is now separated from the optimization part, see slice_code function and TreeSASlicer.\ngreedy_method is removed. If you want to have detailed control of the initializer, please pre-optimize the code with another method and then use :specified to initialize the tree.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.optimize_tree-Union{Tuple{LT}, Tuple{OMEinsumContractionOrders.AbstractEinsum, Dict{LT, Int64}}} where LT","page":"Reference","title":"OMEinsumContractionOrders.optimize_tree","text":"optimize_tree(code, size_dict; βs, ntrials, niters, initializer, score)\n\nOptimize the einsum contraction pattern specified by code, and edge sizes specified by size_dict. Check the docstring of TreeSA for detailed explaination of other input arguments.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.ExactTreewidth","page":"Reference","title":"OMEinsumContractionOrders.ExactTreewidth","text":"const ExactTreewidth = Treewidth{SafeRules{BT, MMW{3}(), MF}}\nExactTreewidth() = Treewidth()\n\nExactTreewidth is a specialization of Treewidth for the SafeRules preprocessing algorithm with the BT elimination algorithm. The BT algorithm is an exact solver for the treewidth problem that implemented in TreeWidthSolver.jl.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.Treewidth","page":"Reference","title":"OMEinsumContractionOrders.Treewidth","text":"struct Treewidth{EL <: EliminationAlgorithm, GM} <: CodeOptimizer\nTreewidth(; alg::EL = SafeRules(BT(), MMW{3}(), MF()))\n\nTree width based solver. The solvers are implemented in CliqueTrees.jl and TreeWidthSolver.jl. They include:\n\nAlgorithm Description Time Complexity Space Complexity\nBFS breadth-first search O(m + n) O(n)\nMCS maximum cardinality search O(m + n) O(n)\nLexBFS lexicographic breadth-first search O(m + n) O(m + n)\nRCMMD reverse Cuthill-Mckee (minimum degree) O(m + n) O(m + n)\nRCMGL reverse Cuthill-Mckee (George-Liu) O(m + n) O(m + n)\nMCSM maximum cardinality search (minimal) O(mn) O(n)\nLexM lexicographic breadth-first search (minimal) O(mn) O(n)\nAMF approximate minimum fill O(mn) O(m + n)\nMF minimum fill O(mn²) -\nMMD multiple minimum degree O(mn²) O(m + n)\n\nDetailed descriptions is available in the CliqueTrees.jl.\n\nFields\n\nalg::EL: The algorithm to use for the treewidth calculation. Available elimination algorithms are listed above.\n\nExample\n\njulia> optimizer = Treewidth();\n\njulia> eincode = OMEinsumContractionOrders.EinCode([['a', 'b'], ['a', 'c', 'd'], ['b', 'c', 'e', 'f'], ['e'], ['d', 'f']], ['a'])\nab, acd, bcef, e, df -> a\n\njulia> size_dict = Dict([c=>(1<<i) for (i,c) in enumerate(['a', 'b', 'c', 'd', 'e', 'f'])]...)\nDict{Char, Int64} with 6 entries:\n  'f' => 64\n  'a' => 2\n  'c' => 8\n  'd' => 16\n  'e' => 32\n  'b' => 4\n\njulia> optcode = optimize_code(eincode, size_dict, optimizer)\nba, ab -> a\n├─ bcf, fac -> ba\n│  ├─ e, bcef -> bcf\n│  │  ├─ e\n│  │  └─ bcef\n│  └─ df, acd -> fac\n│     ├─ df\n│     └─ acd\n└─ ab\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.optimize_treewidth-Union{Tuple{EL}, Tuple{Treewidth{EL}, OMEinsumContractionOrders.AbstractEinsum, Dict}} where EL","page":"Reference","title":"OMEinsumContractionOrders.optimize_treewidth","text":"optimize_treewidth(optimizer, eincode, size_dict)\n\nOptimizing the contraction order via solve the exact tree width of the line graph corresponding to the eincode and return a NestedEinsum object. Check the docstring of treewidth_method for detailed explaination of other input arguments.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.HyperND","page":"Reference","title":"OMEinsumContractionOrders.HyperND","text":"HyperND(;\n    dis = KaHyParND(),\n    algs = (MF(), AMF(), MMD()),\n    level = 6,\n    width = 120,\n    imbalances = 130:130,\n    score = ScoreFunction(),\n)\n\nNested-dissection based optimizer. Recursively partitions a tensor network, then calls a greedy algorithm on the leaves. The optimizer is run a number of times: once for each greedy algorithm in algs and each imbalance value in imbalances. The recursion depth is controlled by the parameters level and width.\n\nThe line graph is partitioned using the algorithm dis. OMEinsumContractionOrders currently supports two partitioning algorithms, both of which require importing an external library.\n\ntype package\nMETISND Metis.jl\nKaHyParND KayHyPar.jl\n\nThe optimizer is implemented using the tree decomposition library CliqueTrees.jl.\n\nArguments\n\ndis: graph partitioning algorithm\nalgs: tuple of elimination algorithms.\nlevel: maximum level\nwidth: minimum width\nimbalances: imbalance parameters \nscore: a function to evaluate the quality of the contraction tree. Default is ScoreFunction().\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.BipartiteResult","page":"Reference","title":"OMEinsumContractionOrders.BipartiteResult","text":"BipartiteResult{RT}\nBipartiteResult(part1, part2, sc, valid)\n\nResult of the bipartite optimization. part1 and part2 are the two parts of the bipartition, sc is the space complexity of the bipartition, valid is a boolean indicating whether the bipartition is valid.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.KaHyParBipartite","page":"Reference","title":"OMEinsumContractionOrders.KaHyParBipartite","text":"KaHyParBipartite{RT,IT,GM}\nKaHyParBipartite(; sc_target, imbalances=collect(0.0:0.005:0.8),\n    max_group_size=40, greedy_config=GreedyMethod())\n\nOptimize the einsum code contraction order using the KaHyPar + Greedy approach. This program first recursively cuts the tensors into several groups using KaHyPar, with maximum group size specifed by max_group_size and maximum space complexity specified by sc_target, Then finds the contraction order inside each group with the greedy search algorithm. Other arguments are\n\nFields\n\nsc_target is the target space complexity, defined as log2(number of elements in the largest tensor),\nimbalances is a KaHyPar parameter that controls the group sizes in hierarchical bipartition,\nmax_group_size is the maximum size that allowed to used greedy search,\nsub_optimizer is the sub-optimizer used to find the contraction order when the group size is small enough.\n\nReferences\n\nHyper-optimized tensor network contraction\nSimulating the Sycamore quantum supremacy circuits\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.SABipartite","page":"Reference","title":"OMEinsumContractionOrders.SABipartite","text":"SABipartite{RT,BT}\nSABipartite(; sc_target=25, ntrials=50, βs=0.1:0.2:15.0, niters=1000\n    max_group_size=40, greedy_config=GreedyMethod(), initializer=:random)\n\nOptimize the einsum code contraction order using the Simulated Annealing bipartition + Greedy approach. This program first recursively cuts the tensors into several groups using simulated annealing, with maximum group size specifed by max_group_size and maximum space complexity specified by sc_target, Then finds the contraction order inside each group with the greedy search algorithm. Other arguments are\n\nFields\n\nsc_target is the target space complexity, defined as log2(number of elements in the largest tensor),\nntrials is the number of repetition (with different random seeds),\nβs is a list of inverse temperature 1/T,\nniters is the number of iteration in each temperature,\nmax_group_size is the maximum size that allowed to used greedy search,\nsub_optimizer is the optimizer for the bipartited sub graphs, one can choose GreedyMethod() or TreeSA(),\ninitializer is the partition configuration initializer, one can choose :random or :greedy (slow but better).\n\nReferences\n\nHyper-optimized tensor network contraction\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.optimize_sa-Tuple{OMEinsumContractionOrders.EinCode, Any}","page":"Reference","title":"OMEinsumContractionOrders.optimize_sa","text":"optimize_sa(code, size_dict; sc_target, max_group_size=40, βs=0.1:0.2:15.0, niters=1000, ntrials=50,\n       sub_optimizer = GreedyMethod(), initializer=:random)\n\nOptimize the einsum code contraction order using the Simulated Annealing bipartition + Greedy approach. size_dict is a dictionary that specifies leg dimensions.  Check the docstring of SABipartite for detailed explaination of other input arguments.\n\nReferences\n\nHyper-optimized tensor network contraction\n\n\n\n\n\n","category":"method"},{"location":"ref/#Slicing","page":"Reference","title":"Slicing","text":"","category":"section"},{"location":"ref/#OMEinsumContractionOrders.TreeSASlicer","page":"Reference","title":"OMEinsumContractionOrders.TreeSASlicer","text":"TreeSASlicer{IT, LT} <: CodeSlicer\n\nA structure for configuring the Tree Simulated Annealing (TreeSA) slicing algorithm. The goal of slicing is to reach the target space complexity specified by score.sc_target.\n\nFields\n\nntrials, βs and niters are annealing parameters, doing ntrials indepedent annealings, each has inverse tempteratures specified by βs, in each temperature, do niters updates of the tree.\nfixed_slices::Vector{LT}: A vector of fixed slices that should not be altered. Default is an empty vector.\noptimization_ratio::Float64: A constant used for determining the number of iterations for slicing. Default is 2.0. i.e. if the current space complexity is 30, and the target space complexity is 20, then the number of iterations for slicing is (30 - 20) x optimization_ratio.\nscore::ScoreFunction: A function to evaluate the quality of the contraction tree. Default is ScoreFunction(sc_target=30.0).\n\nReferences\n\nRecursive Multi-Tensor Contraction for XEB Verification of Quantum Circuits\n\n\n\n\n\n","category":"type"},{"location":"ref/#Preprocessing-code","page":"Reference","title":"Preprocessing code","text":"","category":"section"},{"location":"ref/","page":"Reference","title":"Reference","text":"Some optimizers (e.g. TreeSA) are too costly to run. We provide a preprocessing step to reduce the time of calling optimizers.","category":"page"},{"location":"ref/#OMEinsumContractionOrders.CodeSimplifier","page":"Reference","title":"OMEinsumContractionOrders.CodeSimplifier","text":"CodeSimplifier\n\nAbstract type for code simplifiers.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.MergeGreedy","page":"Reference","title":"OMEinsumContractionOrders.MergeGreedy","text":"MergeGreedy <: CodeSimplifier\nMergeGreedy(; threshhold=-1e-12)\n\nContraction code simplifier (in order to reduce the time of calling optimizers) that merges tensors greedily if the space complexity of merged tensors is reduced (difference smaller than the threshhold).\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.MergeVectors","page":"Reference","title":"OMEinsumContractionOrders.MergeVectors","text":"MergeVectors <: CodeSimplifier\nMergeVectors()\n\nContraction code simplifier (in order to reduce the time of calling optimizers) that merges vectors to closest tensors.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.NetworkSimplifier","page":"Reference","title":"OMEinsumContractionOrders.NetworkSimplifier","text":"NetworkSimplifier{LT}\n\nA network simplifier that contains a list of operations that can be applied to a tensor network to reduce the number of tensors. It is generated from a proprocessor, such as MergeVectors or MergeGreedy.\n\nFields\n\noperations: a list of NestedEinsum objects.\n\n\n\n\n\n","category":"type"},{"location":"ref/#OMEinsumContractionOrders.embed_simplifier-Tuple{OMEinsumContractionOrders.NestedEinsum, OMEinsumContractionOrders.NetworkSimplifier}","page":"Reference","title":"OMEinsumContractionOrders.embed_simplifier","text":"embed_simplifier(code::NestedEinsum, simplifier::NetworkSimplifier)\n\nEmbed the simplifier into the contraction code. A typical workflow is: (i) generate a simplifier with simplify_code, (ii) then optimize the simplified code with optimize_code and (iii) post-process the optimized code with embed_simplifier to produce correct contraction order for the original code. This is automatically done in optimize_code given the simplifier argument is not nothing.\n\nArguments\n\ncode: the contraction code to embed the simplifier into.\nsimplifier: the simplifier to embed, which is a NetworkSimplifier object.\n\nReturns\n\nA new NestedEinsum object.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.simplify_code-Tuple{Union{OMEinsumContractionOrders.EinCode, OMEinsumContractionOrders.NestedEinsum}, Any, MergeVectors}","page":"Reference","title":"OMEinsumContractionOrders.simplify_code","text":"simplify_code(code::Union{EinCode, NestedEinsum}, size_dict, method::CodeSimplifier)\n\nSimplify the contraction code by preprocessing the code with a simplifier.\n\nArguments\n\ncode: the contraction code to simplify.\nsize_dict: the size dictionary of the contraction code.\nmethod: the simplifier to use, which can be MergeVectors or MergeGreedy.\n\nReturns\n\nA tuple of (NetworkSimplifier, newcode), where newcode is a new EinCode object.\n\n\n\n\n\n","category":"method"},{"location":"ref/#Dump-and-load-contraction-orders","page":"Reference","title":"Dump and load contraction orders","text":"","category":"section"},{"location":"ref/#OMEinsumContractionOrders.readjson-Tuple{AbstractString}","page":"Reference","title":"OMEinsumContractionOrders.readjson","text":"readjson(filename::AbstractString)\n\nRead the contraction order from a JSON file.\n\nArguments\n\nfilename: the name of the file to read from.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.writejson-Tuple{AbstractString, Union{OMEinsumContractionOrders.NestedEinsum, OMEinsumContractionOrders.SlicedEinsum}}","page":"Reference","title":"OMEinsumContractionOrders.writejson","text":"writejson(filename::AbstractString, ne::Union{NestedEinsum, SlicedEinsum})\n\nWrite the contraction order to a JSON file.\n\nArguments\n\nfilename: the name of the file to write to.\nne: the contraction order to write. It can be a NestedEinsum or a SlicedEinsum object.\n\n\n\n\n\n","category":"method"},{"location":"ref/#Visualization","page":"Reference","title":"Visualization","text":"","category":"section"},{"location":"ref/","page":"Reference","title":"Reference","text":"Requires using LuxorGraphPlot to load the extension.","category":"page"},{"location":"ref/#OMEinsumContractionOrders.viz_contraction-Tuple","page":"Reference","title":"OMEinsumContractionOrders.viz_contraction","text":"viz_contraction(code::Union{NestedEinsum, SlicedEinsum}; locs=StressLayout(), framerate=10, filename=tempname() * \".mp4\", show_progress=true)\n\nVisualize the contraction process of a tensor network.\n\nArguments\n\ncode: The tensor network to visualize.\n\nKeyword Arguments\n\nlocs: The coordinates or layout algorithm to use for positioning the nodes in the graph. Default is StressLayout().\nframerate: The frame rate of the animation. Default is 10.\nfilename: The name of the output file, with .gif or .mp4 extension. Default is a temporary file with .mp4 extension.\nshow_progress: Whether to show progress information. Default is true.\n\nReturns\n\nthe path of the generated file.\n\n\n\n\n\n","category":"method"},{"location":"ref/#OMEinsumContractionOrders.viz_eins-Tuple","page":"Reference","title":"OMEinsumContractionOrders.viz_eins","text":"viz_eins(code::AbstractEinsum; locs=StressLayout(), filename = nothing, kwargs...)\n\nVisualizes an AbstractEinsum object by creating a tensor network graph and rendering it using GraphViz.\n\nArguments\n\ncode::AbstractEinsum: The AbstractEinsum object to visualize.\n\nKeyword Arguments\n\nlocs=StressLayout(): The coordinates or layout algorithm to use for positioning the nodes in the graph.\nfilename = nothing: The name of the file to save the visualization to. If nothing, the visualization will be displayed on the screen instead of saving to a file.\nconfig = GraphDisplayConfig(): The configuration for displaying the graph. Please refer to the documentation of GraphDisplayConfig for more information.\nkwargs...: Additional keyword arguments to be passed to the GraphViz constructor.\n\n\n\n\n\n","category":"method"},{"location":"#OMEinsumContractionOrders","page":"Home","title":"OMEinsumContractionOrders","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is the documentation for OMEinsumContractionOrders, a Julia package for the optimization of the contraction order of tensor networks.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Installation guide is available in README.md. You can also access its features in OMEinsum, which uses it as the default contraction order optimizer.","category":"page"},{"location":"#Related-Packages","page":"Home","title":"Related Packages","text":"","category":"section"},{"location":"#Derived-packages","page":"Home","title":"Derived packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"OMEinsum.jl: a Julia package for tensor network contraction.\nTensorInference.jl: a Julia package for exact probabilistic inference based on the tensor network representation.\nYao.jl: a Julia package for quantum computing. Its tensor network based simulation backend use OMEinsumContractionOrders.jl as the default contraction order optimizer.\nGenericTensorNetworks.jl: a Julia package for computing solution space properties of computational hard problems. It is based on the generic tensor network method and its contraction order optimization backend is OMEinsumContractionOrders.jl.\nITensorNetworks.jl: a Julia package for physics simulation, which uses OMEinsumContractionOrders.jl as alternative contraction order optimization backend.","category":"page"},{"location":"#Similar-packages","page":"Home","title":"Similar packages","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Cotengra : a python library for contracting tensor networks or einsum expressions involving large numbers of tensors.","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#Example-1:-Optimize-contraction-order-without-backend-specified","page":"Tutorial","title":"Example 1: Optimize contraction order without backend specified","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"OMEinsumContractionOrders can be used as a standalone package to optimize the contraction order of an einsum notation. The first step is to construct an OMEinsumContractionOrders.EinCode object, which is a data type to represent the einsum notation.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using OMEinsumContractionOrders, Graphs, KaHyPar\nfunction random_regular_eincode(n, k; optimize=nothing, seed)\n    g = Graphs.random_regular_graph(n, k; seed)\n    ixs = [[minmax(e.src,e.dst)...] for e in Graphs.edges(g)]  # input indices\n    iy = Int[]  # output indices (scalar output)\n    return OMEinsumContractionOrders.EinCode(ixs, iy)\nend\ncode = random_regular_eincode(100, 3; seed=42);","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here, we define an einsum notation with 3-regular graph topology. The vertices correspond to indices. On each edge, we specify a rank 2 tensor that associates with the vertices it connects. The output is a scalar.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"One can use contraction_complexity function to get the time, space and rewrite cost for contracting this einsum notation. This function takes two arguments: the einsum notation and a dictionary to specify the size of the variables. Here, we use the uniformsize function to specify that all variables have the same size 2. This function returns a dictionary that maps each variable to 2: Dict(i=>2 for i in uniquelabels(code)), where OMEinsumContractionOrders.uniquelabels returns the unique labels in the einsum notation.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"size_dict = uniformsize(code, 2)\ncontraction_complexity(code, size_dict)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Since we do not specify a contraction order, the direct contraction corresponds to brute force enumeration and costs 2^textnumber of vertices operations. No space is required to store the intermediate contraction result and the space complexity is 0. The read-write complexity corresponds to how many element-wise read and write operations are required to perform contraction.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The order of contraction is optimized by the optimize_code function. It takes three arguments: code, size_dict, and optimizer. The optimizer argument is the optimizer to be used. The available optimizers are listed in the optimizers page.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"score = ScoreFunction(sc_target=10, sc_weight=3.0)\noptcode_tree = optimize_code(code, uniformsize(code, 2),\n\tTreeSA(; βs=0.1:0.1:10, ntrials=2, niters=20, score);\n\tslicer=TreeSASlicer(; score)\n    );\ncontraction_complexity(optcode_tree, size_dict)\noptcode_tree.slicing","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The optimize_code function returns the optimized contraction order. The optimized contraction order is a OMEinsumContractionOrders.NestedEinsum object, which is a data type to represent the nested einsum notation.","category":"page"},{"location":"tutorial/#Example-2:-Use-it-in-OMEinsum","page":"Tutorial","title":"Example 2: Use it in OMEinsum","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"OMEinsumContractionOrders is shipped with OMEinsum package, which is a powerful package for tensor network contraction (or einsum contraction). You can use it to optimize the contraction order of an OMEinsum notation.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using OMEinsum\n\ncode = ein\"ij, jk, kl, il->\"\n\noptimized_code = optimize_code(code, uniformsize(code, 2), TreeSA())","category":"page"},{"location":"tutorial/#Example-3:-Visualization","page":"Tutorial","title":"Example 3: Visualization","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The visualization is provided by the LuxorTensorPlot extension. To use it, just load the extension by using LuxorGraphPlot.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"pkg> add OMEinsumContractionOrders, LuxorGraphPlot\n\njulia> using OMEinsumContractionOrders, LuxorGraphPlot","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The extension provides the following two functions, viz_eins and viz_contraction, where the former will plot the tensor network as a graph, and the latter will generate a video or gif of the contraction process.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here is an example:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"julia> using OMEinsumContractionOrders, LuxorGraphPlot\n\njulia> eincode = OMEinsumContractionOrders.EinCode([['a', 'b'], ['a', 'c', 'd'], ['b', 'c', 'e', 'f'], ['e'], ['d', 'f']], ['a'])\nab, acd, bcef, e, df -> a\n\njulia> viz_eins(eincode, filename = \"eins.png\")\n\njulia> nested_eins = optimize_code(eincode, uniformsize(eincode, 2), GreedyMethod())\nab, ab -> a\n├─ ab\n└─ acf, bcf -> ab\n   ├─ acd, df -> acf\n   │  ├─ acd\n   │  └─ df\n   └─ bcef, e -> bcf\n      ├─ bcef\n      └─ e\n\n\njulia> viz_contraction(nested_code)\n[ Info: Generating frames, 7 frames in total\n[ Info: Creating video at: /var/folders/3y/xl2h1bxj4ql27p01nl5hrrnc0000gn/T/jl_SiSvrH/contraction.mp4\n\"/var/folders/3y/xl2h1bxj4ql27p01nl5hrrnc0000gn/T/jl_SiSvrH/contraction.mp4\"","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The resulting image and video will be saved in the current working directory, and the image is shown below:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"<div style=\"text-align:center\">\n\t<img src=\"/assets/eins.png\" alt=\"Image\" width=\"40%\" />\n</div>","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The large white nodes represent the tensors, and the small colored nodes represent the indices, red for closed indices and green for open indices.","category":"page"},{"location":"optimizers/#Choosing-Optimizers","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Supported solvers include:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Optimizer Description\nGreedyMethod Fast, but poor contraction order\nTreeSA Reliable, local search based optimizer [Kalachev2021], but is a bit slow\nHyperND Nested dissection algorithm, similar to KaHyParBipartite. Requires importing either KaHyPar or Metis.\nKaHyParBipartite and SABipartite Graph bipartition based, suited for large tensor networks [Gray2021], requires using KaHyPar package. Alternatively, a simulated annealing bipartition method is provided in SABipartite.\nExactTreewidth (alias of Treewidth{RuleReduction{BT}}) Exact, but takes exponential time [Bouchitté2001], based on package TreeWidthSolver.\nTreewidth Tree width solver based, based on package CliqueTrees, performance is elimination algorithm dependent.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"There is a tradeoff between the time and the quality of the contraction order. The following figure shows the Pareto front of the multi-objective optimization of the time to optimize the contraction order and the time to contract the tensor network.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"(Image: )","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Among these methods, the ExactTreewidth method produces the lowest treewidth, but it does not scale up to tensor networks with more than 50 tensors. The TreeSA is the second best in terms of the treewidth. It works well in most cases, and supports slicing. The only limitation is that it is a bit slow. For application sensitive to overhead, the GreedyMethod and Treewidth method (blue region) are recommended. The Treewidth method is a zoo of methods provided by the package CliqueTrees, which is a collection of methods for finding the approximate tree decomposition of a graph. Most of them have similar performance with the GreedyMethod, and most of them are very efficient. The HyperND method has a very good overall performance in benchmarks (to be added), and it is much faster than the TreeSA method. It relies on the KaHyPar package, which is platform picky.","category":"page"},{"location":"optimizers/#Sec_GreedyMethod","page":"Choosing Optimizers","title":"GreedyMethod","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Implemented as GreedyMethod in the package. The Greedy method is one of the simplest and fastest method for optimizing the contraction order. The idea is to greedily select the pair of tensors with the smallest cost to contract at each step. The cost is defined as:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"L = textsize(textout) - α times (textsize(textin_1) + textsize(textin_2))","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"where textout is the output tensor, and textin_1 and textin_2 are the input tensors. α is a hyperparameter, which is set to 00 by default, meaning that we greedily select the pair of tensors with the smallest size of the output tensor. For alpha = 1, the size increase in each step is greedily optimized.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The greedy method implemented in this package uses the priority queue to select the pair of tensors with the smallest cost to contract at each step. The time complexity is O(n^2 log n) for n tensors, since in each of the n steps, we pick the pair with the smallest cost in O(n log n) time.","category":"page"},{"location":"optimizers/#Sec_TreeSA","page":"Choosing Optimizers","title":"TreeSA","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Implemented as TreeSA in the package. The local search method [Kalachev2021] is a heuristic method based on the idea of simulated annealing. The method starts from a random contraction order and then applies the following four possible transforms as shown in the following figure","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"(Image: )","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"They correspond to the different ways to contract three sub-networks:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"(A * B) * C = (A * C) * B = (C * B) * A \nA * (B * C) = B * (A * C) = C * (B * A)","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"where we slightly abuse the notation \"*\" to denote the tensor contraction, and A B C are the sub-networks to be contracted. Due to the commutative property of the tensor contraction, such transformations do not change the result of the contraction. Even through these transformations are simple, all possible contraction orders can be reached from any initial contraction order. The local search method starts from a random contraction tree. In each step, the above rules are randomly applied to transform the tree and then the cost of the new tree is evaluated, which is defined as","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"mathcalL = texttc + w_s times textsc + w_textrw times textrwc","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"where w_s and w_textrw are the weights of the space complexity and read-write complexity compared to the time complexity, respectively. The optimal choice of weights depends on the specific device and tensor contraction algorithm. One can freely tune the weights to achieve a best performance for their specific problem. Then the transformation is accepted with a probability given by the Metropolis criterion, which is","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"p_textaccept = min(1 e^-beta Delta mathcalL)","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"where beta is the inverse temperature, and Delta mathcalL is the difference of the cost of the new and old contraction trees. During the process, the temperature is gradually decreased, and the process stop when the temperature is low enough.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"This simple algorithm is flexible and works suprisingly well in most cases. Given enough time, it almost always finds the contraction order with the lowest cost. The only weakness is the runtime. It usually takes minutes to optimize a network with 1k tensors. Additionally, the TreeSA method supports using the slicing technique to reduce the space complexity.","category":"page"},{"location":"optimizers/#Sec_HyperND","page":"Choosing Optimizers","title":"HyperND","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Implemented as HyperND in the package.","category":"page"},{"location":"optimizers/#Sec_Bipartite","page":"Choosing Optimizers","title":"KaHyParBipartite and SABipartite","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Implemented as KaHyParBipartite and SABipartite in the package. These two methods are based on the graph bipartition, ","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"min sum_e in textcutomega(e)\ntextst quad c(V_i) leq (1+epsilon) leftlceil fracc(V)2 rightrceil","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"where textcut is the set of hyperedges cut by the partition, omega(e) is the weight of the hyperedge e, V_i is the i-th part of the partition, c(V) is the node weight of the partition, and epsilon is the imbalance parameter. It tries to minimize the cut crossing two blocks, while the size of each block is balanced. The algorithm is implemented in the package KaHyPar.jl[Schlag2021], and the implementation in OMEinsumContractionOrders.jl is mainly based on it.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"<img src=\"https://cloud.githubusercontent.com/assets/484403/25314222/3a3bdbda-2840-11e7-9961-3bbc59b59177.png\" alt=\"alt text\" width=\"50%\" height=\"50%\"><img src=\"https://cloud.githubusercontent.com/assets/484403/25314225/3e061e42-2840-11e7-860c-028a345d1641.png\" alt=\"alt text\" width=\"50%\" height=\"50%\">","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Some platforms may have some issues in installation of KaHyPar, please refer to #12 and #19. The SABipartite is a simulated annealing based alternative to KaHyParBipartite, it can produce similar results while being much more costly.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Note: Benchmarks (to be added) show that the later implementation of HyperND method is better and faster. These two methods are no longer the first choice.","category":"page"},{"location":"optimizers/#Sec_ExactTreewidth","page":"Choosing Optimizers","title":"ExactTreewidth","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Implemented as ExactTreewidth in the package. This method is supported by the Google Summer of Code 2024 project \"Tensor network contraction order optimization and visualization\" released by The Julia Language. In this project, we developed TreeWidthSolver.jl, which implements the Bouchitté–Todinca algorithm[Bouchitté2001]. It later becomes a backend of OMEinsumContracionOrders.jl.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The Bouchitté–Todinca (BT) algorithm [Bouchitté2001] is a method for calculating the treewidth of a graph exactly. It makes use of the theory of minimal triangulations, characterizing the minimal triangulations of a graph via objects called minimal separators and potential maximal cliques of the graph. The BT algorithm has a time complexity of O(Pinm), which are dependent on the graph structure. (TODO: add more details of the algorithm complexity, what is it suited for?).","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The blog post Finding the Optimal Tree Decomposition with Minimal Treewidth - Xuan-Zhao Gao has a more detailed description of this method.","category":"page"},{"location":"optimizers/#Sec_Treewidth","page":"Choosing Optimizers","title":"Treewidth","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Implemented as Treewidth in the package.","category":"page"},{"location":"optimizers/#Exhaustive-Search-(planned)","page":"Choosing Optimizers","title":"Exhaustive Search (planned)","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The exhaustive search [Robert2014] is a method to get the exact optimal contraction complexity. There are three different ways to implement the exhaustive search:","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"Depth-first constructive approach: in each step, choose a pair of tensors to contract a new tensor until all tensors are contracted, and then iterate over all possible contraction sequences without duplication. Note the cheapest contraction sequence thus found.\nBreadth-first constructive approach: the breadth-first method construct the set of intermediate tensors by contracting c tensors (c in 1 n - 1, where n is the number of tensors) in each step, and record the optimal cost for constructing each intermediate tensor. Then in the last step, the optimal cost for contracting all n tensors is obtained.\nDynamic programming: in each step, consider all bipartition that split the tensor network into two parts, if the optimal cost for each part is not recorded, further split them until the cost has been already obtained or only one tensor is left. Then combine the two parts and record the optimal cost of contracting the sub-networks. In this end the optimal cost for the whole network is obtained.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"In more recent work [Robert2014], by reordering the search process in favor of cheapest-first and excluding large numbers of outer product contractions which are shown to be unnecessary, the efficiency of the exhaustive search has been greatly improved. The method has been implemented in TensorOperations.jl.","category":"page"},{"location":"optimizers/#Performance-Benchmark","page":"Choosing Optimizers","title":"Performance Benchmark","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The following figure shows the performance of the different optimizers on the Sycamore 53-20-0 benchmark. This network is for computing the expectation value of a quantum circuit. Its optimal space complexity is known to be 52.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"(Image: )","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"The x-axis (contraction cost) here is defined by the space complexity (the size of the largest intermediate tensor).\nThe y-axis (time) is the time taken to find the optimal contraction order.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"By checking the Pareto front, we can see that the TreeSA, HyperND and GreedyMethod method are among the best. If you want the speed to find the optimal contraction order, the GreedyMethod is the best choice. If you want the quality of the contraction order, the TreeSA and HyperND method are the best choices. HyperND is basically a better version of KaHyParBipartite method, we are going to deprecate the KaHyParBipartite (and also SABipartite) method in the future.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"More benchmarks, and the platform details can be found in the benchmark repo. Or just check this full report: Benchmark Report.","category":"page"},{"location":"optimizers/#References","page":"Choosing Optimizers","title":"References","text":"","category":"section"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"[Bouchitté2001]: Bouchitté, V., Todinca, I., 2001. Treewidth and Minimum Fill-in: Grouping the Minimal Separators. SIAM J. Comput. 31, 212–232. https://doi.org/10.1137/S0097539799359683","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"[Gray2021]: Gray, Johnnie, and Stefanos Kourtis. \"Hyper-optimized tensor network contraction.\" Quantum 5 (2021): 410.","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"[Kalachev2021]: Kalachev, Gleb, Pavel Panteleev, and Man-Hong Yung. \"Recursive multi-tensor contraction for XEB verification of quantum circuits.\" arXiv preprint arXiv:2108.05665 (2021).","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"[Robert2014]: Pfeifer, R.N.C., Haegeman, J., Verstraete, F., 2014. Faster identification of optimal contraction sequences for tensor networks. Phys. Rev. E 90, 033315. https://doi.org/10.1103/PhysRevE.90.033315","category":"page"},{"location":"optimizers/","page":"Choosing Optimizers","title":"Choosing Optimizers","text":"[Schlag2021]: Schlag, S., Heuer, T., Gottesbüren, L., Akhremtsev, Y., Schulz, C., Sanders, P., 2021. High-Quality Hypergraph Partitioning. https://doi.org/10.48550/arXiv.2106.08696","category":"page"}]
}
